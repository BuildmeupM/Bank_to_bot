import streamlit as st

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ page config ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Streamlit ‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Streamlit ‡πÉ‡∏î‡πÜ)
st.set_page_config(
    page_title="Bank PDF to Excel Converter",
    page_icon="üè¶",
    layout="wide"
)

import pandas as pd
import pdfplumber
from datetime import datetime, timedelta
import io
import re
import zipfile
from typing import Dict, List, Tuple, Optional, Any
import os
import sys
import importlib.util
import importlib.machinery
import importlib
import time
import logging
import asyncio
import subprocess
from concurrent.futures import ThreadPoolExecutor

try:
    from NewPeak import NewPeakBot
except ImportError:
    NewPeakBot = None

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ logger ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö config)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import config
try:
    import config
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Link_conpany ‡πÅ‡∏•‡∏∞ Link_receipt ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if not hasattr(config, 'Link_conpany'):
        logger.warning("‚ö†Ô∏è config module ‡πÑ‡∏°‡πà‡∏°‡∏µ Link_conpany attribute")
    else:
        logger.info(f"‚úÖ ‡∏û‡∏ö Link_conpany ‡πÉ‡∏ô config: {getattr(config, 'Link_conpany', None)}")
    if not hasattr(config, 'Link_receipt'):
        logger.warning("‚ö†Ô∏è config module ‡πÑ‡∏°‡πà‡∏°‡∏µ Link_receipt attribute")
    else:
        logger.info(f"‚úÖ ‡∏û‡∏ö Link_receipt ‡πÉ‡∏ô config: {getattr(config, 'Link_receipt', None)}")
except ImportError:
    config = None
    logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import config ‡πÑ‡∏î‡πâ")

# ‡πÄ‡∏Å‡πá‡∏ö bot instances ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô module level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô garbage collection
_peakengine_bots = []
_newpeak_bots = []

# ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ asyncio event loop ‡∏ö‡∏ô Windows ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö subprocess (Playwright)
if sys.platform.startswith("win"):
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    except Exception:
        pass

# Import DBDDataWarehouseBot ‡∏à‡∏≤‡∏Å bot_data.py
try:
    # ‡∏•‡∏ö cache ‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏´‡∏°‡πà
    modules_to_remove = [key for key in sys.modules.keys() if 'bot_data' in key.lower()]
    for module_name in modules_to_remove:
        try:
            del sys.modules[module_name]
        except:
            pass
    
    # ‡∏•‡∏ö __pycache__ ‡∏î‡πâ‡∏ß‡∏¢
    import shutil
    import inspect
    current_dir = os.path.dirname(os.path.abspath(__file__))
    cache_dir = os.path.join(current_dir, '__pycache__')
    if os.path.exists(cache_dir):
        try:
            shutil.rmtree(cache_dir, ignore_errors=True)
        except:
            pass
    
    # Import ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏¢‡∏Å
    bot_data_path = os.path.join(current_dir, 'bot_data.py')
    
    if os.path.exists(bot_data_path):
        # ‡πÉ‡∏ä‡πâ unique module name ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        module_name = f"bot_data_module_{int(time.time() * 1000)}"
        spec = importlib.util.spec_from_file_location(module_name, bot_data_path)
        bot_data_module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = bot_data_module
        spec.loader.exec_module(bot_data_module)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ class ‡∏°‡∏µ use_browser parameter ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        sig = inspect.signature(bot_data_module.DBDDataWarehouseBot.__init__)
        params = list(sig.parameters.keys())
        
        if 'use_browser' not in params:
            raise AttributeError(f"bot_data.py ‡πÑ‡∏°‡πà‡∏°‡∏µ use_browser parameter ‡πÉ‡∏ô __init__ (‡∏û‡∏ö parameters: {params})")
        
        DBDDataWarehouseBot = bot_data_module.DBDDataWarehouseBot
        create_dbd_summary_table = bot_data_module.create_dbd_summary_table
        
        logger.info(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î bot_data.py ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (parameters: {params})")
    else:
        # Fallback
        logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå bot_data.py ‡∏ó‡∏µ‡πà {bot_data_path}")
        from bot_data import DBDDataWarehouseBot, create_dbd_summary_table
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
        sig = inspect.signature(DBDDataWarehouseBot.__init__)
        params = list(sig.parameters.keys())
        if 'use_browser' not in params:
            raise AttributeError(f"bot_data module ‡πÑ‡∏°‡πà‡∏°‡∏µ use_browser parameter (‡∏û‡∏ö: {params})")
        
except Exception as e:
    error_msg = str(e)
    logger.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î bot_data module ‡πÑ‡∏î‡πâ: {error_msg}")
    st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î bot_data module ‡πÑ‡∏î‡πâ: {error_msg}")
    st.error(f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå bot_data.py ‡∏°‡∏µ use_browser parameter ‡πÉ‡∏ô __init__")
    st.error(f"Path ‡∏ó‡∏µ‡πà‡∏•‡∏≠‡∏á‡∏´‡∏≤: {bot_data_path if 'bot_data_path' in locals() else '‡πÑ‡∏°‡πà‡∏û‡∏ö'}")
    
    # Fallback: ‡∏™‡∏£‡πâ‡∏≤‡∏á class ‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dummy (‡πÅ‡∏ï‡πà‡∏°‡∏µ use_browser)
    class DBDDataWarehouseBot:
        def __init__(self, use_browser=False, headless=False):
            st.error(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î bot_data.py ‡πÑ‡∏î‡πâ: {error_msg}")
            st.stop()
        
        def search_company_info(self, company_name, log_callback=None):
            return {"error": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î bot_data module ‡πÑ‡∏î‡πâ"}
        
        def format_company_info(self, company_info):
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    
    def create_dbd_summary_table(df):
        return pd.DataFrame()

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏≠‡∏ó‡∏Å‡∏±‡∏ö Streamlit
def integrate_with_streamlit(df: pd.DataFrame, company_column: str = '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•',
                              use_browser: bool = False, headless: bool = False) -> pd.DataFrame:
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö Streamlit ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bot instance ‡∏û‡∏£‡πâ‡∏≠‡∏° browser mode
    bot = DBDDataWarehouseBot(use_browser=use_browser, headless=headless)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
    df['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD'] = ""
    df['‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD'] = ""
    address_column_map = [
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà', 'address_house_no'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô', 'address_village'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà', 'address_moo'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•', 'address_subdistrict'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', 'address_district'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', 'address_province'),
        ('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå', 'address_postal_code')
    ]

    for column_name, _ in address_column_map:
        if column_name not in df.columns:
            df[column_name] = ""
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á progress bar ‡πÅ‡∏•‡∏∞ status
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    stats_container = st.container()
    with stats_container:
        col1, col2, col3, col4 = st.columns(4)
        success_count = col1.metric("‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", "0")
        error_count = col2.metric("‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", "0")
        not_found_count = col3.metric("üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "0")
        total_count = col4.metric("üìä ‡∏£‡∏ß‡∏°", "0")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á container ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
    log_container = st.container()
    with log_container:
        st.subheader("üìã ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏≠‡∏ó")
        log_expander = st.expander("üîç ‡∏î‡∏π‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", expanded=False)
        log_messages = []
        log_placeholder = log_expander.empty()
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
    total_companies = len(df[df[company_column].notna() & (df[company_column] != '')])
    processed_count = 0
    success_stats = 0
    error_stats = 0
    not_found_stats = 0
    
    def log_callback(message, status="info"):
        """Callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á log"""
        log_messages.append({
            "message": message,
            "status": status,
            "time": datetime.now().strftime("%H:%M:%S")
        })
        
        # ‡πÅ‡∏™‡∏î‡∏á log ‡πÉ‡∏ô expander
        log_text = ""
        for log in log_messages[-50:]:  # ‡πÅ‡∏™‡∏î‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 50 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            icon = {
                "info": "‚ÑπÔ∏è",
                "success": "‚úÖ",
                "warning": "‚ö†Ô∏è",
                "error": "‚ùå"
            }.get(log["status"], "üìù")
            
            log_text += f"[{log['time']}] {icon} {log['message']}\n"
        
        log_placeholder.code(log_text, language=None)
    
    for index, row in df.iterrows():
        company_name = row[company_column]
        
        if pd.isna(company_name) or not str(company_name).strip():
            continue
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï status
        processed_count += 1
        progress = processed_count / total_companies
        progress_bar.progress(progress)
        
        status_text.text(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {processed_count}/{total_companies}: {company_name}")
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏û‡∏£‡πâ‡∏≠‡∏° log callback)
        company_info = bot.search_company_info(str(company_name), log_callback=log_callback)
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        formatted_info = bot.format_company_info(company_info)
        df.at[index, '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD'] = formatted_info
        if isinstance(company_info, dict):
            if company_info.get("company_name"):
                df.at[index, '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD'] = company_info.get("company_name")

            for column_name, key_name in address_column_map:
                if key_name in company_info:
                    df.at[index, column_name] = company_info.get(key_name, "")
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        if "error" in company_info:
            error_stats += 1
            st.warning(f"‚ö†Ô∏è {company_name}: {company_info['error']}")
        elif formatted_info == "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•":
            not_found_stats += 1
            st.info(f"üîç {company_name}: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        else:
            success_stats += 1
            st.success(f"‚úÖ {company_name}: ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï metrics
        success_count.metric("‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", str(success_stats))
        error_count.metric("‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", str(error_stats))
        not_found_count.metric("üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", str(not_found_stats))
        total_count.metric("üìä ‡∏£‡∏ß‡∏°", str(processed_count))
        
        # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
        time.sleep(0.5)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    st.markdown("---")
    st.subheader("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", success_stats, delta=f"{success_stats/total_companies*100:.1f}%")
    with col2:
        st.metric("‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", error_stats, delta=f"{error_stats/total_companies*100:.1f}%")
    with col3:
        st.metric("üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", not_found_stats, delta=f"{not_found_stats/total_companies*100:.1f}%")
    
    # ‡∏•‡πâ‡∏≤‡∏á progress bar ‡πÅ‡∏•‡∏∞ status
    progress_bar.empty()
    status_text.empty()
    
    return df

def create_dbd_summary_table(df: pd.DataFrame) -> pd.DataFrame:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"""
    if '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD' not in df.columns:
        return pd.DataFrame()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
    summary_data = []
    
    for index, row in df.iterrows():
        dbd_info = row.get('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD', '')
        company_name = row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', '')
        db_company_name = row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD', '')
        
        if dbd_info and dbd_info != "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" and "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î" not in dbd_info:
            # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            info_parts = dbd_info.split(' | ')
            
            summary_row = {
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó': company_name,
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD': db_company_name,
                '‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô': '',
                '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞': '',
                '‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô': '',
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà': '',
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', ''),
                '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå': row.get('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå', '')
            }
            
            for part in info_parts:
                if '‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:' in part:
                    summary_row['‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô'] = part.replace('‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:', '').strip()
                elif '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à:' in part:
                    summary_row['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à'] = part.replace('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à:', '').strip()
                elif '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞:' in part:
                    summary_row['‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞'] = part.replace('‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞:', '').strip()
                elif '‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:' in part:
                    summary_row['‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô'] = part.replace('‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:', '').strip()
                elif '‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà:' in part:
                    summary_row['‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà'] = part.replace('‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà:', '').strip()
            
            summary_data.append(summary_row)
    
    return pd.DataFrame(summary_data)


def test_playwright_browser(url: str = "https://datawarehouse.dbd.go.th/index") -> bool:
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î Playwright Chromium ‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á CLI"""
    try:
        command = [sys.executable, "-m", "playwright", "open", url]
        subprocess.Popen(command)
        return True
    except FileNotFoundError:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á playwright CLI ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `pip install playwright` ‡πÅ‡∏•‡∏∞ `playwright install chromium`")
    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î Playwright Browser ‡πÑ‡∏î‡πâ: {e}")
    return False

def open_peakengine_login() -> bool:
    """‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PeakEngine Login ‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏Å username/password ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    try:
        # Reload config module ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        global config
        if config is not None:
            try:
                import importlib
                importlib.reload(config)
                logger.info("üîÑ Reload config module ‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ reload config ‡πÑ‡∏î‡πâ: {e}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ config ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if config is None:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.py")
            return False
        
        # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö attributes ‡πÉ‡∏ô config
        try:
            attrs = [attr for attr in dir(config) if not attr.startswith('_')]
            logger.info(f"üîç Attributes ‡πÉ‡∏ô config (‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô): {', '.join(attrs)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö attributes ‡πÑ‡∏î‡πâ: {e}")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å config
        url = getattr(config, 'PEAKENGINE_LOGIN_URL', 'https://secure.peakengine.com/Home/Login')
        username = getattr(config, 'PEAKENGINE_USERNAME', '')
        password = getattr(config, 'PEAKENGINE_PASSWORD', '')
        headless = getattr(config, 'HEADLESS_MODE', False)
        
        # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏•‡∏¥‡∏á‡∏Ñ‡πå
        link_company_check = getattr(config, 'Link_conpany', None)
        link_receipt_check = getattr(config, 'Link_receipt', None)
        logger.info(f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - Link_conpany: {repr(link_company_check)}, Link_receipt: {repr(link_receipt_check)}")
        
        if not username or not password:
            st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å username ‡πÅ‡∏•‡∏∞ password ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå config.py")
            # ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            command = [sys.executable, "-m", "playwright", "open", url]
            subprocess.Popen(command)
            return True
        
        # ‡πÉ‡∏ä‡πâ PeakEngineBot class ‡πÅ‡∏ó‡∏ô (‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ browser lifecycle ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)
        try:
            from peakengine_bot import PeakEngineBot
            
            def run_bot():
                """‡∏£‡∏±‡∏ô bot ‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å"""
                bot = None
                try:
                    # Reload config module ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                    try:
                        import importlib
                        import config as config_module
                        importlib.reload(config_module)
                        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï config reference
                        global config
                        config = config_module
                        logger.info("üîÑ Reload config module ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ reload config ‡πÑ‡∏î‡πâ: {e}")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bot instance
                    bot = PeakEngineBot(use_browser=True, headless=headless)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö bot instance ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô module-level list ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô garbage collection
                    global _peakengine_bots
                    _peakengine_bots.append(bot)
                    logger.info(f"üìù ‡πÄ‡∏Å‡πá‡∏ö bot instance ‡πÑ‡∏ß‡πâ (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(_peakengine_bots)})")
                    
                    # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö attributes ‡πÉ‡∏ô config
                    try:
                        attrs = [attr for attr in dir(config) if not attr.startswith('_')]
                        logger.info(f"üîç Attributes ‡πÉ‡∏ô config: {', '.join(attrs)}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö attributes ‡πÑ‡∏î‡πâ: {e}")
                    
                    # ‡∏≠‡πà‡∏≤‡∏ô‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏à‡∏≤‡∏Å config
                    link_company = getattr(config, 'Link_conpany', None)
                    link_receipt = getattr(config, 'Link_receipt', None)
                    
                    # Debug: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ
                    logger.info(f"üîç link_company = {repr(link_company)}")
                    logger.info(f"üîç link_receipt = {repr(link_receipt)}")
                    
                    if link_company:
                        logger.info(f"üìñ ‡∏≠‡πà‡∏≤‡∏ô Link_conpany ‡∏à‡∏≤‡∏Å config: {link_company}")
                    else:
                        logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Link_conpany ‡πÉ‡∏ô config.py")
                        # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                        try:
                            if hasattr(config, 'Link_conpany'):
                                link_company = config.Link_conpany
                                logger.info(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô Link_conpany ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ: {link_company}")
                            else:
                                logger.warning("‚ö†Ô∏è config ‡πÑ‡∏°‡πà‡∏°‡∏µ attribute Link_conpany")
                        except Exception as e:
                            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô Link_conpany: {e}")
                    
                    if link_receipt:
                        logger.info(f"üìñ ‡∏≠‡πà‡∏≤‡∏ô Link_receipt ‡∏à‡∏≤‡∏Å config: {link_receipt}")
                    else:
                        logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Link_receipt ‡πÉ‡∏ô config.py")
                        # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                        try:
                            if hasattr(config, 'Link_receipt'):
                                link_receipt = config.Link_receipt
                                logger.info(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô Link_receipt ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÑ‡∏î‡πâ: {link_receipt}")
                            else:
                                logger.warning("‚ö†Ô∏è config ‡πÑ‡∏°‡πà‡∏°‡∏µ attribute Link_receipt")
                        except Exception as e:
                            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô Link_receipt: {e}")
                    
                    # ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö, ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login
                    def log_callback(message, status="info"):
                        logger.info(f"[{status.upper()}] {message}")
                    
                    success = bot.open_login_page_and_fill(username, password, link_company=link_company, link_receipt=link_receipt, log_callback=log_callback)
                    
                    if success:
                        logger.info("‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö, ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                        logger.info("üëÄ Browser ‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
                    else:
                        logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡∏´‡∏£‡∏∑‡∏≠ navigate ‡πÑ‡∏î‡πâ - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log")
                    
                    # ‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î browser ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
                    # bot.close()  # ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å close() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ browser ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà
                    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: bot instance ‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô _peakengine_bots ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô garbage collection
                    
                except Exception as e:
                    logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
                    logger.error(f"Error details: {e}", exc_info=True)
                    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‡πÅ‡∏•‡∏∞ bot ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î browser
                    # ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ browser ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
            
            # ‡∏£‡∏±‡∏ô‡πÉ‡∏ô thread ‡πÅ‡∏¢‡∏Å
            executor = ThreadPoolExecutor(max_workers=1)
            executor.submit(run_bot)
            
            return True
            
        except ImportError:
            logger.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö peakengine_bot.py - ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏õ‡∏¥‡∏î browser ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤‡πÅ‡∏ó‡∏ô")
            # Fallback: ‡πÄ‡∏õ‡∏¥‡∏î browser ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
            command = [sys.executable, "-m", "playwright", "open", url]
            subprocess.Popen(command)
            return True
        
    except FileNotFoundError:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á playwright CLI ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `pip install playwright` ‡πÅ‡∏•‡∏∞ `playwright install chromium`")
    except ImportError:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö playwright library ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `pip install playwright`")
    except Exception as e:
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PeakEngine ‡πÑ‡∏î‡πâ: {e}")
        logger.error(f"Error details: {e}", exc_info=True)
    return False


def open_newpeak_login() -> bool:
    """‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PEAK Account (‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà) ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ Login ‡∏î‡πâ‡∏ß‡∏¢ NewPeakBot"""
    if NewPeakBot is None:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™ NewPeakBot (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå NewPeak.py ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)")
        logger.error("NewPeakBot ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô - ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡∏î‡∏π‡∏• NewPeak")
        return False

    if config is None:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.py ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö NewPeak")
        return False

    username = getattr(config, "NEWPEAK_USERNAME", "")
    if not username:
        username = getattr(config, "PEAKENGINE_USERNAME", "")
    password = getattr(config, "NEWPEAK_PASSWORD", "")
    if not password:
        password = getattr(config, "PEAKENGINE_PASSWORD", "")
    headless = getattr(config, "HEADLESS_MODE", False)

    if not username or not password:
        st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î NEWPEAK_USERNAME / NEWPEAK_PASSWORD (‡∏´‡∏£‡∏∑‡∏≠ PEAKENGINE_USERNAME / PEAKENGINE_PASSWORD) ‡πÉ‡∏ô config.py")
        return False

    def run_bot():
        bot = None
        try:
            bot = NewPeakBot(use_browser=True, headless=headless)
            _newpeak_bots.append(bot)
            logger.info(f"üÜï ‡πÄ‡∏Å‡πá‡∏ö NewPeakBot instance (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(_newpeak_bots)} ‡∏ï‡∏±‡∏ß)")

            def log_callback(message: str, status: str = "info"):
                log_func = {
                    "info": logger.info,
                    "success": logger.info,
                    "warning": logger.warning,
                    "error": logger.error,
                }.get(status, logger.info)
                log_func(f"[NewPeakBot] {message}")

            login_success = bot.login(
                username,
                password,
                navigate_after_login=True,
                log_callback=log_callback,
            )

            if login_success:
                logger.info("‚úÖ NewPeakBot Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Browser ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ")
            else:
                logger.warning("‚ö†Ô∏è NewPeakBot Login ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô config.py")
        except Exception as exc:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô NewPeakBot: {exc}", exc_info=True)
            if bot and bot._executor:  # type: ignore[attr-defined]
                try:
                    bot.close()
                except Exception:
                    pass

    executor = ThreadPoolExecutor(max_workers=1)
    executor.submit(run_bot)
    return True


def wait_for_newpeak_login(bot, timeout: float = 60.0, poll_interval: float = 0.5, log_callback=None) -> bool:
    """‡∏£‡∏≠‡πÉ‡∏´‡πâ NewPeakBot login ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"""
    start = time.time()
    while time.time() - start < timeout:
        if getattr(bot, "is_logged_in", False):
            return True
        time.sleep(poll_interval)
    if log_callback:
        try:
            log_callback("‚ö†Ô∏è ‡∏£‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö New Peak ‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î", "warning")
        except Exception:
            pass
    return getattr(bot, "is_logged_in", False)


def wait_for_newpeak_instance(timeout: float = 30.0, poll_interval: float = 0.5):
    """‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot (‡∏à‡∏≤‡∏Å thread ‡∏≠‡∏∑‡πà‡∏ô)"""
    start = time.time()
    while time.time() - start < timeout:
        if _newpeak_bots:
            bot = _newpeak_bots[-1]
            if bot:
                return bot
        time.sleep(poll_interval)
    return _newpeak_bots[-1] if _newpeak_bots else None

class BankPDFReader:
    """‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
    
    def __init__(self):
        self.bank_configs = self.load_bank_configs()
    
    def load_bank_configs(self) -> Dict:
        """‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£"""
        return {
            "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            },
            "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            },
            "‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            },
            "‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            },
            "TMB": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            },
            "‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï": {
                "patterns": {
                    "date": r"(\d{2}/\d{2}/\d{4})",
                    "amount": r"([\d,]+\.\d{2})",
                    "description": r"([A-Za-z0-9\s\-\.]+)",
                    "balance": r"([\d,]+\.\d{2})"
                },
                "columns": ["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô", "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠"]
            }
        }
    
    def extract_text_from_pdf(self, pdf_file) -> str:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF"""
        try:
            with pdfplumber.open(pdf_file) as pdf:
                text = ""
                page_texts = []
                for i, page in enumerate(pdf.pages):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                        page_texts.append({
                            "page_number": i + 1,
                            "text": page_text,
                            "char_count": len(page_text),
                            "line_count": len(page_text.split('\n'))
                        })
                
                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
                self.pdf_pages_info = page_texts
                return text
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF: {str(e)}")
            return ""
    
    def extract_tables_from_pdf(self, pdf_file) -> List:
        """‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF"""
        try:
            tables = []
            with pdfplumber.open(pdf_file) as pdf:
                for i, page in enumerate(pdf.pages):
                    page_tables = page.extract_tables()
                    if page_tables:
                        for j, table in enumerate(page_tables):
                            tables.append({
                                "page_number": i + 1,
                                "table_number": j + 1,
                                "table_data": table,
                                "row_count": len(table),
                                "col_count": len(table[0]) if table else 0
                            })
            return tables
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á: {str(e)}")
            return []
    
    def analyze_kbank_statement(self, text: str) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢"""
        analysis = {
            "account_info": {},
            "transaction_patterns": [],
            "date_ranges": [],
            "amount_patterns": [],
            "keywords": []
        }
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
        account_patterns = {
            "account_number": r"‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ç‡∏ä‡∏µ[:\s]*(\d+)",
            "account_name": r"‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ[:\s]*([^\n]+)",
            "account_type": r"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ö‡∏±‡∏ç‡∏ä‡∏µ[:\s]*([^\n]+)",
            "branch": r"‡∏™‡∏≤‡∏Ç‡∏≤[:\s]*([^\n]+)"
        }
        
        for key, pattern in account_patterns.items():
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                analysis["account_info"][key] = match.group(1).strip()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        date_patterns = [
            r"(\d{1,2}/\d{1,2}/\d{4})",
            r"(\d{1,2}-\d{1,2}-\d{4})",
            r"(\d{4}-\d{1,2}-\d{1,2})"
        ]
        
        for pattern in date_patterns:
            dates = re.findall(pattern, text)
            analysis["date_ranges"].extend(dates)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô
        amount_patterns = [
            r"([\d,]+\.\d{2})",
            r"([\d,]+\.\d{2})",
            r"([\d,]+)"
        ]
        
        for pattern in amount_patterns:
            amounts = re.findall(pattern, text)
            analysis["amount_patterns"].extend(amounts)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        keywords = [
            "‡∏ñ‡∏≠‡∏ô", "‡∏ù‡∏≤‡∏Å", "‡πÇ‡∏≠‡∏ô", "‡∏ä‡∏≥‡∏£‡∏∞", "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ", "‡∏£‡∏≤‡∏¢‡∏à‡πà‡∏≤‡∏¢",
            "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠", "‡∏¢‡∏≠‡∏î‡∏¢‡∏Å‡∏°‡∏≤", "‡∏¢‡∏≠‡∏î‡∏¢‡∏Å‡πÑ‡∏õ", "‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢",
            "‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "ATM", "POS"
        ]
        
        for keyword in keywords:
            if keyword in text:
                analysis["keywords"].append(keyword)
        
        return analysis
    
    def parse_bank_statement(self, text: str, bank_name: str) -> pd.DataFrame:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡πÄ‡∏õ‡πá‡∏ô DataFrame"""
        if bank_name == "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢":
            return self.parse_kbank_statement(text)
        else:
            return self.parse_generic_statement(text, bank_name)
    
    def parse_kbank_statement(self, text: str) -> pd.DataFrame:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô DataFrame"""
        lines = text.split('\n')
        transactions = []
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
        account_info = self.extract_account_info(text)
        
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
        previous_balance = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πÄ‡∏ß‡∏•‡∏≤ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô ‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
            # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: 01-10-25 11:17 ‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏° 51.43 22,127,753.64 ‡πÇ‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤/‡∏´‡∏±‡∏Å‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥...
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD-MM-YY ‡∏´‡∏£‡∏∑‡∏≠ DD/MM/YYYY)
            date_match = re.search(r'(\d{2}[-/]\d{2}[-/]\d{2,4})', line)
            if date_match:
                date = date_match.group(1)
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ß‡∏•‡∏≤ (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö HH:MM)
                time_match = re.search(r'(\d{2}:\d{2})', line)
                time = time_match.group(1) if time_match else ""
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö 123,456.78)
                amount_matches = re.findall(r'([\d,]+\.\d{2})', line)
                
                if len(amount_matches) >= 2:
                    amount = amount_matches[0]  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏£‡∏Å
                    balance = amount_matches[1]  # ‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                elif len(amount_matches) == 1:
                    amount = amount_matches[0]
                    balance = ""
                else:
                    amount = ""
                    balance = ""
                
                # ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
                parts = line.split()
                transaction_type = ""
                description = ""
                
                # ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô
                amount_pos = -1
                for i, part in enumerate(parts):
                    if re.match(r'[\d,]+\.\d{2}', part):
                        amount_pos = i
                        break
                
                # ‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà-‡πÄ‡∏ß‡∏•‡∏≤ ‡∏Å‡∏±‡∏ö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô) - ‡πÄ‡∏≠‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡∏≠‡∏Å
                if amount_pos > 2:  # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà ‡πÄ‡∏ß‡∏•‡∏≤ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                    # ‡∏Ç‡πâ‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                    transaction_type = " ".join(parts[2:amount_pos])
                    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                    transaction_type = re.sub(r'\d{2}:\d{2}\s*', '', transaction_type).strip()
                
                # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ (‡∏´‡∏•‡∏±‡∏á‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
                if len(amount_matches) >= 2:
                    balance_pos = -1
                    for i, part in enumerate(parts):
                        if part == balance:
                            balance_pos = i
                            break
                    
                    if balance_pos > -1 and balance_pos < len(parts) - 1:
                        description = " ".join(parts[balance_pos + 1:])
                
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô
                amount_display = amount
                if amount and balance and previous_balance:
                    try:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                        current_balance = float(balance.replace(',', ''))
                        prev_balance = float(previous_balance.replace(',', ''))
                        amount_value = float(amount.replace(',', ''))
                        
                        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏•‡∏î‡∏•‡∏á ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏¥‡∏î‡∏•‡∏ö
                        if current_balance < prev_balance:
                            amount_display = f"({amount})"
                    except:
                        pass
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≠‡∏î‡∏•‡∏ö
                if transaction_type and any(keyword in transaction_type.lower() for keyword in ['‡∏Ñ‡πà‡∏≤‡∏ò‡∏£‡∏£‡∏°‡πÄ‡∏ô‡∏µ‡∏¢‡∏°', 'fee', 'charge', 'commission']):
                    if amount and not amount_display.startswith('('):
                        amount_display = f"({amount})"
                
                transactions.append({
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": date,
                    "‡πÄ‡∏ß‡∏•‡∏≤": time,
                    "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£": transaction_type,
                    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô": amount_display,
                    "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠": balance,
                    "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢": description
                })
                
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
                if balance:
                    previous_balance = balance
        
        return pd.DataFrame(transactions)
    
    def extract_account_info(self, text: str) -> Dict:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        account_info = {}
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
        account_match = re.search(r'‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡∏ù‡∏≤‡∏Å\s*(\d+-\d+-\d+-\d+)', text)
        if account_match:
            account_info['account_number'] = account_match.group(1)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
        name_match = re.search(r'‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ\s*([^\n]+)', text)
        if name_match:
            account_info['account_name'] = name_match.group(1).strip()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏≤‡∏Ç‡∏≤
        branch_match = re.search(r'‡∏™‡∏≤‡∏Ç‡∏≤‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á‡∏ö‡∏±‡∏ç‡∏ä‡∏µ\s*([^\n]+)', text)
        if branch_match:
            account_info['branch'] = branch_match.group(1).strip()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        period_match = re.search(r'‡∏£‡∏≠‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\s*(\d{2}/\d{2}/\d{4})\s*-\s*(\d{2}/\d{2}/\d{4})', text)
        if period_match:
            account_info['period_start'] = period_match.group(1)
            account_info['period_end'] = period_match.group(2)
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏¢‡∏≠‡∏î‡∏¢‡∏Å‡πÑ‡∏õ
        balance_match = re.search(r'‡∏¢‡∏≠‡∏î‡∏¢‡∏Å‡πÑ‡∏õ\s*([\d,]+\.\d{2})', text)
        if balance_match:
            account_info['opening_balance'] = balance_match.group(1)
        
        return account_info
    
    def classify_transfer_type(self, description: str) -> str:
        """‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°"""
        if not description:
            return "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
        
        description_lower = description.lower()
        
        # ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)
        company_keywords = ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ö‡∏à‡∏Å', 'company', 'co.', 'ltd', 'limited']
        if any(keyword in description_lower for keyword in company_keywords):
            return "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)"
        
        # ‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)
        partnership_keywords = ['‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô', '‡∏´‡∏à‡∏Å', 'partnership']
        if any(keyword in description_lower for keyword in partnership_keywords):
            return "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)"
        
        # ‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
        person_keywords = ['‡∏ô‡∏≤‡∏¢', '‡∏ô‡∏≤‡∏á', '‡∏ô.‡∏™.', 'miss', 'mr', 'mrs', 'ms', '‡∏ô‡∏™.']
        if any(keyword in description_lower for keyword in person_keywords):
            return "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• (‡∏ä‡∏∑‡πà‡∏≠-‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•)
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
        if re.search(r'[‡∏Å-‡πô]+\s+[‡∏Å-‡πô]+', description) and len(description.split()) <= 3:
            return "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"
        
        return "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"
    
    def extract_entity_name(self, description: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢"""
        if not description:
            return ""
        
        description_lower = description.lower()
        
        # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢
        patterns = [
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ...
            r'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏Å‡∏£|\s+‡∏à‡∏≥‡∏Å‡∏±‡∏î|\s+‡∏°‡∏´‡∏≤‡∏ä‡∏ô|\s+‡∏Ø‡∏•‡∏Ø|\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ö‡∏à‡∏Å. ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ...
            r'‡∏ö‡∏à‡∏Å\.\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏Å‡∏£|\s+‡∏à‡∏≥‡∏Å‡∏±‡∏î|\s+‡∏°‡∏´‡∏≤‡∏ä‡∏ô|\s+‡∏Ø‡∏•‡∏Ø|\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡πâ‡∏≤‡∏á ...
            r'‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏Å‡∏£|\s+‡∏à‡∏≥‡∏Å‡∏±‡∏î|\s+‡∏°‡∏´‡∏≤‡∏ä‡∏ô|\s+‡∏Ø‡∏•‡∏Ø|\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ô‡∏≤‡∏¢ ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'‡∏ô‡∏≤‡∏¢\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ô‡∏≤‡∏á ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'‡∏ô‡∏≤‡∏á\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ô.‡∏™. ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'‡∏ô\.‡∏™\.\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... ‡∏ô‡∏™. ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'‡∏ô‡∏™\.\s+([‡∏Å-‡πôA-Za-z0-9\s\.\-\+]+?)(?:\s+‡∏à‡∏≤‡∏Å|\s+‡∏ñ‡∏∂‡∏á|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... Mr. ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'mr\.?\s+([A-Za-z0-9\s\.\-\+]+?)(?:\s+from|\s+to|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... Miss ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'miss\s+([A-Za-z0-9\s\.\-\+]+?)(?:\s+from|\s+to|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... Mrs. ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'mrs\.?\s+([A-Za-z0-9\s\.\-\+]+?)(?:\s+from|\s+to|\s+$|$)',
            # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: ... Ms. ‡∏ä‡∏∑‡πà‡∏≠ ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏• ...
            r'ms\.?\s+([A-Za-z0-9\s\.\-\+]+?)(?:\s+from|\s+to|\s+$|$)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, description, re.IGNORECASE)
            if match:
                entity_name = match.group(1).strip()
                
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠
                entity_name = re.sub(r'\s+', ' ', entity_name)  # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
                
                # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                entity_name = re.sub(r'\b‡∏ö‡∏à‡∏Å\.?\b', '', entity_name, flags=re.IGNORECASE)  # ‡∏•‡∏ö "‡∏ö‡∏à‡∏Å."
                entity_name = re.sub(r'\b‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó\b', '', entity_name, flags=re.IGNORECASE)  # ‡∏•‡∏ö "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó"
                entity_name = re.sub(r'\b‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô\b', '', entity_name, flags=re.IGNORECASE)  # ‡∏•‡∏ö "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô"
                entity_name = re.sub(r'\b‡∏à‡∏≥‡∏Å‡∏±‡∏î\b', '', entity_name, flags=re.IGNORECASE)  # ‡∏•‡∏ö "‡∏à‡∏≥‡∏Å‡∏±‡∏î"
                entity_name = re.sub(r'\b‡∏°‡∏´‡∏≤‡∏ä‡∏ô\b', '', entity_name, flags=re.IGNORECASE)  # ‡∏•‡∏ö "‡∏°‡∏´‡∏≤‡∏ä‡∏ô"
                
                # ‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
                entity_name = re.sub(r'\+\+', '', entity_name)  # ‡∏•‡∏ö "++"
                entity_name = re.sub(r'\+', '', entity_name)  # ‡∏•‡∏ö "+"
                entity_name = re.sub(r'‡∏Ø‡∏•‡∏Ø', '', entity_name)  # ‡∏•‡∏ö "‡∏Ø‡∏•‡∏Ø"
                
                # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©
                entity_name = re.sub(r'^[0-9\-\+\.\s]+', '', entity_name)
                
                # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                entity_name = re.sub(r'\s+', ' ', entity_name)  # ‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥
                entity_name = entity_name.strip()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
                if len(entity_name) <= 100 and len(entity_name) > 0:
                    return entity_name
        
        return ""
    
    def format_date_column(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dd/MM/yyyy"""
        if df.empty or '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' not in df.columns:
            return df
        
        df_formatted = df.copy()
        
        def convert_date_format(date_str):
            if not date_str or pd.isna(date_str):
                return date_str
            
            try:
                # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢
                date_patterns = [
                    # DD-MM-YY (‡πÄ‡∏ä‡πà‡∏ô 01-10-25)
                    r'(\d{2})-(\d{2})-(\d{2})',
                    # DD/MM/YYYY (‡πÄ‡∏ä‡πà‡∏ô 01/10/2025)
                    r'(\d{2})/(\d{2})/(\d{4})',
                    # DD-MM-YYYY (‡πÄ‡∏ä‡πà‡∏ô 01-10-2025)
                    r'(\d{2})-(\d{2})-(\d{4})',
                    # YYYY-MM-DD (‡πÄ‡∏ä‡πà‡∏ô 2025-10-01)
                    r'(\d{4})-(\d{2})-(\d{2})',
                ]
                
                for pattern in date_patterns:
                    match = re.match(pattern, str(date_str).strip())
                    if match:
                        if len(match.groups()) == 3:
                            day, month, year = match.groups()
                            
                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏µ 2 ‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏´‡∏•‡∏±‡∏Å
                            if len(year) == 2:
                                year_int = int(year)
                                if year_int >= 0 and year_int <= 30:  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ 00-30 ‡πÄ‡∏õ‡πá‡∏ô 2000-2030
                                    year = f"20{year}"
                                else:  # 31-99 ‡πÄ‡∏õ‡πá‡∏ô 1931-1999
                                    year = f"19{year}"
                            
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dd/MM/yyyy
                            return f"{day.zfill(2)}/{month.zfill(2)}/{year}"
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏î‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
                return date_str
                
            except Exception:
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°
                return date_str
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        df_formatted['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'] = df_formatted['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].apply(convert_date_format)
        
        return df_formatted
    
    def create_transfer_summary(self, df: pd.DataFrame) -> pd.DataFrame:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô"""
        if df.empty or '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' not in df.columns:
            return pd.DataFrame()
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢)
        df['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] = df['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(self.classify_transfer_type)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
        df['‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•'] = df['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(self.extract_entity_name)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
        summary_data = []
        
        for transfer_type in df['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'].unique():
            type_data = df[df['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] == transfer_type]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            count = len(type_data)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°
            total_amount = 0
            positive_amount = 0
            negative_amount = 0
            
            for amount in type_data['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô']:
                if amount and amount != "":
                    try:
                        # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                        clean_amount = amount.replace(',', '').replace('(', '').replace(')', '')
                        amount_value = float(clean_amount)
                        
                        if '(' in amount and ')' in amount:
                            # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏•‡∏ö
                            total_amount -= amount_value
                            negative_amount += amount_value
                        else:
                            # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ö‡∏ß‡∏Å
                            total_amount += amount_value
                            positive_amount += amount_value
                    except:
                        pass
            
            summary_data.append({
                '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô': transfer_type,
                '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': count,
                '‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°': f"{total_amount:,.2f}",
                '‡∏¢‡∏≠‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°': f"{positive_amount:,.2f}",
                '‡∏¢‡∏≠‡∏î‡∏•‡∏î': f"{negative_amount:,.2f}",
                '‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞': f"{(count/len(df)*100):.1f}%"
            })
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢)
        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', ascending=False)
        
        return summary_df
    
    def parse_generic_statement(self, text: str, bank_name: str) -> pd.DataFrame:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô DataFrame"""
        config = self.bank_configs.get(bank_name, self.bank_configs["‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢"])
        
        lines = text.split('\n')
        transactions = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            date_match = re.search(config["patterns"]["date"], line)
            if date_match:
                date = date_match.group(1)
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô
                amount_match = re.search(config["patterns"]["amount"], line)
                amount = amount_match.group(1) if amount_match else ""
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
                description_match = re.search(config["patterns"]["description"], line)
                description = description_match.group(1) if description_match else ""
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠
                balance_match = re.search(config["patterns"]["balance"], line)
                balance = balance_match.group(1) if balance_match else ""
                
                transactions.append({
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": date,
                    "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£": description,
                    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô": amount,
                    "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠": balance
                })
        
        return pd.DataFrame(transactions)
    
    def detect_bank(self, text: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        bank_keywords = {
            "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢": ["‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢", "Kasikorn", "KBank"],
            "‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û": ["‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û", "Bangkok Bank", "BBL"],
            "‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ": ["‡∏Å‡∏£‡∏∏‡∏á‡∏®‡∏£‡∏µ", "Krungsri", "Bank of Ayudhya"],
            "‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢": ["‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢", "Krung Thai", "KTB"],
            "TMB": ["TMB", "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢"],
            "‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï": ["‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï", "Thanachart", "TBank"]
        }
        
        text_lower = text.lower()
        for bank, keywords in bank_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    return bank
        
        return "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢"  # default

def process_pdf_file(uploaded_file, reader, selected_bank):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå PDF ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"""
    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå PDF..."):
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF
        text = reader.extract_text_from_pdf(uploaded_file)
        
        if text:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            detected_bank = reader.detect_bank(text)
            st.info(f"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£: {detected_bank}")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            st.header("üîç ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å PDF")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
            if hasattr(reader, 'pdf_pages_info'):
                st.subheader("üìÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤")
                for page_info in reader.pdf_pages_info:
                    with st.expander(f"‡∏´‡∏ô‡πâ‡∏≤ {page_info['page_number']} ({page_info['char_count']} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£, {page_info['line_count']} ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)"):
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                        lines = page_info['text'].split('\n')
                        if lines:
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
                            raw_data = []
                            for i, line in enumerate(lines):
                                if line.strip():
                                    raw_data.append({
                                        "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î": i + 1,
                                        "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤": line.strip()
                                    })
                            
                            if raw_data:
                                df_raw = pd.DataFrame(raw_data)
                                st.dataframe(df_raw, use_container_width=True, height=300)
                            else:
                                st.text(page_info['text'])
            
            # ‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏à‡∏≤‡∏Å PDF
            tables = reader.extract_tables_from_pdf(uploaded_file)
            if tables:
                st.subheader("üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô PDF")
                for table_info in tables:
                    with st.expander(f"‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà {table_info['table_number']} ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ {table_info['page_number']} ({table_info['row_count']} ‡πÅ‡∏ñ‡∏ß, {table_info['col_count']} ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå)"):
                        if table_info['table_data']:
                            df_table = pd.DataFrame(table_info['table_data'])
                            st.dataframe(df_table, use_container_width=True)
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢
            if detected_bank == "‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢":
                st.subheader("üè¶ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏Å‡∏™‡∏¥‡∏Å‡∏£‡πÑ‡∏ó‡∏¢")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
                account_info = reader.extract_account_info(text)
                if account_info:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏±‡∏ç‡∏ä‡∏µ:**")
                        for key, value in account_info.items():
                            if key == 'account_number':
                                st.write(f"- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {value}")
                            elif key == 'account_name':
                                st.write(f"- ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ: {value}")
                            elif key == 'branch':
                                st.write(f"- ‡∏™‡∏≤‡∏Ç‡∏≤: {value}")
                            elif key == 'period_start':
                                st.write(f"- ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {value}")
                            elif key == 'period_end':
                                st.write(f"- ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: {value}")
                            elif key == 'opening_balance':
                                st.write(f"- ‡∏¢‡∏≠‡∏î‡∏¢‡∏Å‡πÑ‡∏õ: {value}")
                    
                    with col2:
                        st.write("**‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**")
                        st.write(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(text.split('\n'))}")
                        st.write(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£: {len(text)}")
                        
                        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°
                        transaction_lines = []
                        for line in text.split('\n'):
                            if re.search(r'\d{2}[-/]\d{2}[-/]\d{2,4}', line):
                                transaction_lines.append(line)
                        
                        st.write(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö: {len(transaction_lines)}")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
                analysis = reader.analyze_kbank_statement(text)
                
                if analysis["date_ranges"]:
                    st.write(f"**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏û‡∏ö:** {len(analysis['date_ranges'])} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                    unique_dates = list(set(analysis["date_ranges"]))[:10]
                    st.write(f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {', '.join(unique_dates)}")
                
                if analysis["amount_patterns"]:
                    st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö:** {len(analysis['amount_patterns'])} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                    unique_amounts = list(set(analysis["amount_patterns"]))[:10]
                    st.write(f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {', '.join(unique_amounts)}")
                
                if analysis["keywords"]:
                    st.write(f"**‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏û‡∏ö:** {', '.join(analysis['keywords'])}")
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            df = reader.parse_bank_statement(text, detected_bank)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô dd/MM/yyyy
            df = reader.format_date_column(df)

            def parse_amount_value(amount_str):
                if amount_str is None or (isinstance(amount_str, float) and pd.isna(amount_str)):
                    return None
                text_amount = str(amount_str).strip()
                if not text_amount:
                    return None
                negative = False
                if text_amount.startswith('(') and text_amount.endswith(')'):
                    negative = True
                    text_amount = text_amount[1:-1]
                text_amount = text_amount.replace(',', '').replace('+', '').strip()
                try:
                    value = float(text_amount)
                    return -value if negative else value
                except ValueError:
                    return None

            if not df.empty and '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô' in df.columns:
                df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] = df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô'].apply(parse_amount_value)
            else:
                df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] = pd.Series(dtype=float)

            if not df.empty and '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in df.columns:
                df['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] = df['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(reader.classify_transfer_type)
                st.subheader("üè∑Ô∏è ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö")
                category_counts = df['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'].value_counts()
                category_summary = pd.DataFrame({
                    '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô': category_counts.index,
                    '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': category_counts.values
                })
                st.dataframe(category_summary, use_container_width=True, hide_index=True)

            if not df.empty and '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in df.columns:
                income_df = df[df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] > 0]
                expense_df = df[df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] < 0]
                st.subheader("üí∞ ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤/‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏≠‡∏Å")
                col_in, col_out, col_net = st.columns(3)
                with col_in:
                    st.metric("‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ (Income)", f"{income_df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'].sum():,.2f}", help="‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å")
                    st.caption(f"‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤: {len(income_df)}")
                with col_out:
                    st.metric("‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏≠‡∏Å (Expense)", f"{expense_df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'].sum():,.2f}", help="‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏ö")
                    st.caption(f"‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏≠‡∏Å: {len(expense_df)}")
                with col_net:
                    net_amount = df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'].sum()
                    st.metric("‡∏¢‡∏≠‡∏î‡∏™‡∏∏‡∏ó‡∏ò‡∏¥", f"{net_amount:,.2f}", help="‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ - ‡πÄ‡∏á‡∏¥‡∏ô‡∏≠‡∏≠‡∏Å")
            
            if not df.empty:
                st.success("‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                if '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in df.columns:
                    st.info("üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dd/MM/yyyy ‡πÅ‡∏•‡πâ‡∏ß")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                st.header("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏µ (‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏ß‡∏•‡∏≤)
                display_columns = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô', '‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠']
                if '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in df.columns:
                    display_columns.append('‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢')
                if '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô' in df.columns:
                    display_columns.append('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô')
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
                if '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in df.columns:
                    df_display = df.copy()
                    df_display['‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•'] = df_display['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(reader.extract_entity_name)
                    if '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•' in df_display.columns:
                        display_columns.append('‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•')
                else:
                    df_display = df
                
                available_columns = [col for col in display_columns if col in df_display.columns]
                
                st.dataframe(
                    df_display[available_columns], 
                    use_container_width=True, 
                    height=400,
                    column_config={
                        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": st.column_config.TextColumn(
                            "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                            help="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dd/MM/yyyy)"
                        ),
                        "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô": st.column_config.TextColumn(
                            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
                            help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° (‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏•‡∏î‡∏•‡∏á)"
                        ),
                        "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•": st.column_config.TextColumn(
                            "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                            help="‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° (‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢)"
                        )
                    }
                )
                
                # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", len(df))
                with col2:
                    st.metric("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].min() if '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in df.columns else "N/A")
                with col3:
                    st.metric("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î", df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].max() if '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in df.columns else "N/A")
                with col4:
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏î
                    if '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in df.columns and not df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'].isna().all():
                        total_amount = df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'].sum()
                        positive_count = int((df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] > 0).sum())
                        negative_count = int((df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] < 0).sum())
                        
                        st.metric("‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°", f"{total_amount:,.2f}")
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
                        st.write(f"üìà ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°: {positive_count}")
                        st.write(f"üìâ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏î: {negative_count}")
                    else:
                        st.metric("‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°", "N/A")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                if '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£' in df.columns:
                    st.subheader("üìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
                    transaction_summary = df['‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£'].value_counts()
                    st.bar_chart(transaction_summary)
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô
                if '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in df.columns:
                    st.subheader("üè¢ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
                    transfer_summary = reader.create_transfer_summary(df.copy())
                    
                    if not transfer_summary.empty:
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
                        st.dataframe(
                            transfer_summary,
                            use_container_width=True,
                            column_config={
                                "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô": st.column_config.TextColumn(
                                    "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô",
                                    help="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏¥‡∏ô"
                                ),
                                "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£": st.column_config.NumberColumn(
                                    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£",
                                    help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°"
                                ),
                                "‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°": st.column_config.TextColumn(
                                    "‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°",
                                    help="‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
                                ),
                                "‡∏¢‡∏≠‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°": st.column_config.TextColumn(
                                    "‡∏¢‡∏≠‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°",
                                    help="‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤"
                                ),
                                "‡∏¢‡∏≠‡∏î‡∏•‡∏î": st.column_config.TextColumn(
                                    "‡∏¢‡∏≠‡∏î‡∏•‡∏î",
                                    help="‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏•‡∏á"
                                ),
                                "‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞": st.column_config.TextColumn(
                                    "‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞",
                                    help="‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£"
                                )
                            }
                        )
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write("**üìä ‡∏Å‡∏£‡∏≤‡∏ü‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£:**")
                            chart_data = transfer_summary.set_index('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô')['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£']
                            st.bar_chart(chart_data)
                        
                        with col2:
                            st.write("**üí∞ ‡∏Å‡∏£‡∏≤‡∏ü‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°:**")
                            # ‡πÅ‡∏õ‡∏•‡∏á‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü
                            chart_amounts = []
                            for amount_str in transfer_summary['‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°']:
                                try:
                                    amount_value = float(amount_str.replace(',', ''))
                                    chart_amounts.append(amount_value)
                                except:
                                    chart_amounts.append(0)
                            
                            chart_df = pd.DataFrame({
                                '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô': transfer_summary['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'],
                                '‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°': chart_amounts
                            }).set_index('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô')
                            
                            st.bar_chart(chart_df['‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°'])
                        
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                        st.subheader("üìã ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó")
                        
                        for transfer_type in transfer_summary['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô']:
                            type_count = transfer_summary[transfer_summary['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] == transfer_type]['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£'].iloc[0]
                            type_total = transfer_summary[transfer_summary['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] == transfer_type]['‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°'].iloc[0]
                            
                            with st.expander(f"üîç {transfer_type} ({type_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£, ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°: {type_total})"):
                                type_data = df[df['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(reader.classify_transfer_type) == transfer_type]
                                
                                if not type_data.empty:
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏¢‡πà‡∏≠‡∏¢
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", type_count)
                                    with col2:
                                        st.metric("‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°", type_total)
                                    with col3:
                                        st.metric("‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞", transfer_summary[transfer_summary['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] == transfer_type]['‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞'].iloc[0])
                                    
                                    st.markdown("---")
                                    
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°
                                    st.write(f"**‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ({type_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£):**")
                                    
                                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                                        if '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in type_data.columns:
                                            unique_dates = sorted(type_data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].unique())
                                            selected_dates = st.multiselect(
                                                "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:",
                                                unique_dates,
                                                default=unique_dates,
                                                key=f"date_filter_{transfer_type}"
                                            )
                                            if selected_dates:
                                                type_data = type_data[type_data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].isin(selected_dates)]
                                    
                                    with col2:
                                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô
                                        amount_filter = st.selectbox(
                                            "‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô:",
                                            ["‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏î"],
                                            key=f"amount_filter_{transfer_type}"
                                        )
                                        if '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in type_data.columns:
                                            if amount_filter == "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°":
                                                type_data = type_data[type_data['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] > 0]
                                            elif amount_filter == "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏î":
                                                type_data = type_data[type_data['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] < 0]
                                        else:
                                            if amount_filter == "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°":
                                                type_data = type_data[type_data['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô'].str.contains(r'^\d', na=False)]
                                            elif amount_filter == "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏î":
                                                type_data = type_data[type_data['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô'].str.contains(r'^\(', na=False)]
                                    
                                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á
                                    display_columns = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô', '‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠']
                                    if '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in type_data.columns:
                                        display_columns.append('‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢')
                                    if '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•' in type_data.columns:
                                        display_columns.append('‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•')
                                    
                                    available_columns = [col for col in display_columns if col in type_data.columns]
                                    
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß
                                    filtered_count = len(type_data)
                                    if filtered_count != type_count:
                                        st.info(f"üìä ‡πÅ‡∏™‡∏î‡∏á {filtered_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {type_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
                                    
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
                                    st.dataframe(
                                        type_data[available_columns], 
                                        use_container_width=True,
                                        height=400,
                                        column_config={
                                            "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà": st.column_config.TextColumn(
                                                "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                                                help="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dd/MM/yyyy)"
                                            ),
                                            "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£": st.column_config.TextColumn(
                                                "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£",
                                                help="‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°"
                                            ),
                                            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô": st.column_config.TextColumn(
                                                "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
                                                help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô (‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏ö‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏•‡∏î‡∏•‡∏á)"
                                            ),
                                            "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠": st.column_config.TextColumn(
                                                "‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠",
                                                help="‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°"
                                            ),
                                            "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢": st.column_config.TextColumn(
                                                "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
                                                help="‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°"
                                            ),
                                            "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•": st.column_config.TextColumn(
                                                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                                                help="‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° (‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢)"
                                            )
                                        }
                                    )
                                    
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠‡∏¢
                                    st.markdown("---")
                                    st.write("**‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠‡∏¢:**")
                                    
                                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
                                    if '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in type_data.columns:
                                        positive_count = int((type_data['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] > 0).sum())
                                        negative_count = int((type_data['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] < 0).sum())
                                    else:
                                        positive_count = len(type_data[type_data['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô'].str.contains(r'^\d', na=False)])
                                        negative_count = len(type_data[type_data['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô'].str.contains(r'^\(', na=False)])
                                    
                                    col1, col2, col3, col4 = st.columns(4)
                                    with col1:
                                        st.write(f"üìà ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°: {positive_count}")
                                    with col2:
                                        st.write(f"üìâ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏î: {negative_count}")
                                    with col3:
                                        st.write(f"üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°: {type_data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].min()}")
                                    with col4:
                                        st.write(f"üìÖ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: {type_data['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].max()}")
                                        
                                else:
                                    st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                    else:
                        st.write("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á (‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏î)
                if '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô' in df.columns:
                    st.subheader("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á")
                    
                    # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
                    positive_transactions = df[df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] > 0] if '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in df.columns else pd.DataFrame()
                    negative_transactions = df[df['‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric'] < 0] if '‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric' in df.columns else pd.DataFrame()
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**üìà ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°:**")
                        if not positive_transactions.empty:
                            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏ß‡∏•‡∏≤)
                            display_columns = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô', '‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠']
                            available_columns = [col for col in display_columns if col in positive_transactions.columns]
                            st.dataframe(positive_transactions[available_columns], use_container_width=True)
                        else:
                            st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°")
                    
                    with col2:
                        st.write("**üìâ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏•‡∏î:**")
                        if not negative_transactions.empty:
                            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÄ‡∏ß‡∏•‡∏≤)
                            display_columns = ['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô', '‡∏¢‡∏≠‡∏î‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠']
                            available_columns = [col for col in display_columns if col in negative_transactions.columns]
                            st.dataframe(negative_transactions[available_columns], use_container_width=True)
                        else:
                            st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡∏•‡∏î")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                if '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà' in df.columns:
                    st.subheader("üìÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà")
                    date_summary = df['‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà'].value_counts().sort_index()
                    st.line_chart(date_summary)
                
                # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel
                st.header("üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö
                    output_raw = io.BytesIO()
                    with pd.ExcelWriter(output_raw, engine='openpyxl') as writer:
                        df.to_excel(writer, sheet_name='Bank_Statement', index=False)
                    
                    output_raw.seek(0)
                    
                    st.download_button(
                        label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö",
                        data=output_raw.getvalue(),
                        file_name=f"bank_statement_raw_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        help="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó",
                        key="download_raw_pdf"
                    )
                
                with col2:
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏•‡πâ‡∏ß
                    if '‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢' in df.columns:
                        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
                        df_classified = df.copy()
                        df_classified['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] = df_classified['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(reader.classify_transfer_type)
                        df_classified['‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•'] = df_classified['‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢'].apply(reader.extract_entity_name)
                        
                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ
                        transfer_summary = reader.create_transfer_summary(df.copy())
                        
                        output_classified = io.BytesIO()
                        with pd.ExcelWriter(output_classified, engine='openpyxl') as writer:
                            # Sheet 1: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏•‡πâ‡∏ß
                            df_classified.to_excel(writer, sheet_name='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏•‡πâ‡∏ß', index=False)
                            
                            # Sheet 2: ‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                            if not transfer_summary.empty:
                                transfer_summary.to_excel(writer, sheet_name='‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó', index=False)
                            
                            # Sheet 3: ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô
                            for transfer_type in df_classified['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'].unique():
                                type_data = df_classified[df_classified['‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô'] == transfer_type]
                                sheet_name = f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó_{transfer_type}"[:31]  # Excel sheet name limit
                                type_data.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        output_classified.seek(0)
                        
                        st.download_button(
                            label="üìä ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÅ‡∏•‡πâ‡∏ß",
                            data=output_classified.getvalue(),
                            file_name=f"bank_statement_classified_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            help="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó",
                            key="download_classified_pdf"
                        )
                    else:
                        st.info("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢")
            else:
                st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                st.subheader("üìù ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                lines = text.split('\n')
                raw_data = []
                for i, line in enumerate(lines):
                    if line.strip():
                        raw_data.append({
                            "‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î": i + 1,
                            "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤": line.strip()
                        })
                
                if raw_data:
                    df_raw = pd.DataFrame(raw_data)
                    st.dataframe(df_raw, use_container_width=True, height=400)
                else:
                    st.text_area("‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å PDF:", text, height=400)
        else:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡πÑ‡∏î‡πâ")

def render_statement_page(reader, selected_bank):
    """‡∏´‡∏ô‡πâ‡∏≤ Statement - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PDF ‡πÅ‡∏•‡∏∞ Excel ‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£"""
    st.header("üìÑ Statement - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Statement ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£")
    st.markdown("---")
    
    # ‡πÅ‡∏ó‡πá‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î PDF ‡πÅ‡∏•‡∏∞ Excel
    tab1, tab2 = st.tabs(["üìÑ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF", "üìä ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel"])
    
    uploaded_file = None
    
    with tab1:
        st.write("**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£**")
        
        # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF
        st.subheader("üìÅ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF")
        uploaded_file = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£",
            type=['pdf'],
            help="‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ç‡∏≠‡∏á‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            key="pdf_upload_statement"
        )
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î
        if uploaded_file is not None:
            st.success(f"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {uploaded_file.name}")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå", f"{uploaded_file.size / 1024:.1f} KB")
            with col2:
                st.metric("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå", uploaded_file.type)
            with col3:
                st.metric("‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", selected_bank)
            
            # ‡∏õ‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            if st.button("üîÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå PDF", type="primary", key="process_pdf_btn"):
                process_pdf_file(uploaded_file, reader, selected_bank)
    
    with tab2:
        st.write("**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó**")
        
        # ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel
        st.subheader("üìä ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel")
        uploaded_excel = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel",
            type=['xlsx', 'xls'],
            help="‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
            key="excel_upload_statement"
        )
        
        if uploaded_excel is not None:
            try:
                # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel
                df_excel = pd.read_excel(uploaded_excel)
                
                st.success("‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                st.subheader("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
                st.write(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {len(df_excel)}")
                st.write(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå: {len(df_excel.columns)}")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà:**")
                st.write(", ".join(df_excel.columns.tolist()))
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
                company_columns = [col for col in df_excel.columns if any(keyword in col.lower() for keyword in ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ä‡∏∑‡πà‡∏≠', 'company', 'name'])]
                
                if company_columns:
                    selected_column = st.selectbox(
                        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:",
                        company_columns,
                        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                        key="company_column_statement"
                    )
                    
                    if selected_column:
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                        st.subheader("üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
                        st.dataframe(df_excel.head(10), use_container_width=True)
                        
                        # ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            if st.button("üîç ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏à‡∏≤‡∏Å Excel", type="primary", key="fetch_dbd_excel_statement"):
                                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
                                st.subheader("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD DataWarehouse")
                                
                                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
                                st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:** {len(df_excel[df_excel[selected_column].notna() & (df_excel[selected_column] != '')])}")
                                st.write(f"**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** {selected_column}")
                                
                                # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                                st.write("**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:**")
                                sample_data = df_excel[df_excel[selected_column].notna() & (df_excel[selected_column] != '')][selected_column].head(5)
                                for i, company in enumerate(sample_data, 1):
                                    st.write(f"{i}. {company}")
                                
                                st.markdown("---")
                                
                                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD DataWarehouse..."):
                                    try:
                                        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD
                                        use_browser_mode = st.session_state.get('use_browser_mode', True)
                                        headless_mode = st.session_state.get('headless_mode', False)
                                        
                                        df_excel_with_dbd = integrate_with_streamlit(
                                            df_excel.copy(), 
                                            selected_column,
                                            use_browser=use_browser_mode,
                                            headless=headless_mode
                                        )
                                        
                                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                                        st.success("‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD DataWarehouse ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                                        
                                        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
                                        dbd_summary = create_dbd_summary_table(df_excel_with_dbd)
                                        
                                        if not dbd_summary.empty:
                                            st.subheader("üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD")
                                            st.dataframe(dbd_summary, use_container_width=True)
                                        
                                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
                                        output_dbd = io.BytesIO()
                                        with pd.ExcelWriter(output_dbd, engine='openpyxl') as writer:
                                            df_excel_with_dbd.to_excel(writer, sheet_name='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD', index=False)
                                            
                                            if not dbd_summary.empty:
                                                dbd_summary.to_excel(writer, sheet_name='‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD', index=False)
                                        
                                        output_dbd.seek(0)
                                        
                                        st.download_button(
                                            label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD",
                                            data=output_dbd.getvalue(),
                                            file_name=f"excel_with_dbd_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                            help="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Excel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD DataWarehouse",
                                            key="download_dbd_excel_statement"
                                        )
                                        
                                    except Exception as e:
                                        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}")
                        
                        with col2:
                            st.info("""
                            **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:**
                            ‚Ä¢ ‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
                            ‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 0.5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
                            ‚Ä¢ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å [DBD DataWarehouse](https://datawarehouse.dbd.go.th/index)
                            """)
                else:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                    st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà:**")
                    for col in df_excel.columns:
                        st.write(f"‚Ä¢ {col}")
            
            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel: {str(e)}")
    
    return uploaded_file, selected_bank

def render_dbd_bot_page(reader):
    """‡∏´‡∏ô‡πâ‡∏≤ Bot ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡πå - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD"""
    st.header("ü§ñ Bot ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡πå")
    st.markdown("---")
    st.write("**‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD DataWarehouse**")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
    use_browser_mode = st.session_state.get('use_browser_mode', True)
    headless_mode = st.session_state.get('headless_mode', False)
    
    if use_browser_mode:
        if headless_mode:
            st.info("üåê ‡πÇ‡∏´‡∏°‡∏î: Chromium Browser (Headless) - Browser ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏ã‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠")
        else:
            st.success("üåê ‡πÇ‡∏´‡∏°‡∏î: Chromium Browser (‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠) - üëÄ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Chromium window ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")
    else:
        st.info("üì° ‡πÇ‡∏´‡∏°‡∏î: Requests Mode - ‡πÉ‡∏ä‡πâ requests library ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤")
    
    st.markdown("---")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    mode = st.radio(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î:",
        ["üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß", "üìä ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel"],
        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏≠‡∏ó DBD",
        key="dbd_bot_mode"
    )
    
    if mode == "üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß":
        st.subheader("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó")
        
        # ‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        company_name = st.text_input(
            "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:",
            placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ó‡∏£‡∏≠‡πÄ‡∏ß‡∏•‡∏•‡πå ‡∏Å‡∏£",
            help="‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤",
            key="company_search_input"
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_button = st.button("üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", type="primary", use_container_width=True, key="search_company_btn")
        
        with col2:
            if st.button("üßπ ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", use_container_width=True, key="clear_search_btn"):
                st.rerun()
        
        if search_button and company_name:
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ browser mode
            if use_browser_mode and not headless_mode:
                st.info("üëÄ **‡∏î‡∏π Chromium Browser ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà** - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå!")
            
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
                log_messages = []
                log_expander = st.expander("üîç ‡∏î‡∏π‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", expanded=True)
                
                def log_callback(message, status="info"):
                    log_messages.append({
                        "message": message,
                        "status": status,
                        "time": datetime.now().strftime("%H:%M:%S")
                    })
                    
                    # ‡πÅ‡∏™‡∏î‡∏á log
                    log_text = ""
                    for log in log_messages:
                        icon = {
                            "info": "‚ÑπÔ∏è",
                            "success": "‚úÖ",
                            "warning": "‚ö†Ô∏è",
                            "error": "‚ùå"
                        }.get(log["status"], "üìù")
                        log_text += f"[{log['time']}] {icon} {log['message']}\n"
                    
                    log_expander.code(log_text, language=None)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á bot instance
                try:
                    if use_browser_mode:
                        st.info("üöÄ **‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î Chromium Browser...**")
                        st.info("üëÄ **Browser ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà - ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏π Browser window!**")
                    
                    bot = DBDDataWarehouseBot(use_browser=use_browser_mode, headless=headless_mode)
                    
                    if use_browser_mode and bot.browser:
                        st.success("‚úÖ **Browser ‡πÄ‡∏õ‡∏¥‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!**")
                        st.info("üëÄ **‡∏î‡∏π Browser window ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå!**")
                        st.markdown("---")
                except Exception as e:
                    st.error(f"‚ùå **‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î Browser ‡πÑ‡∏î‡πâ:** {str(e)}")
                    st.warning("‚ö†Ô∏è **‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ Requests Mode ‡πÅ‡∏ó‡∏ô** (‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ JavaScript protection)")
                    st.info("üí° **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**")
                    st.code("pip install playwright\nplaywright install chromium", language="bash")
                    # ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ requests mode
                    bot = DBDDataWarehouseBot(use_browser=False, headless=False)
                
                # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏û‡∏£‡πâ‡∏≠‡∏° log callback)
                company_info = bot.search_company_info(company_name, log_callback=log_callback)
                
                if "error" in company_info:
                    st.error(f"‚ùå {company_info['error']}")
                else:
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î 1148-1418)
                    st.success("‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó!")
                    st.markdown("---")
                    
                    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ô: ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ===
                    header_col1, header_col2, header_col3 = st.columns([2, 2, 1])
                    with header_col1:
                        if company_info.get("company_name"):
                            st.markdown(f"**‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:** {company_info.get('company_name')}")
                    with header_col2:
                        if company_info.get("registration_number"):
                            st.markdown(f"**‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:** {company_info.get('registration_number')}")
                    with header_col3:
                        st.markdown(f"**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ì ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:** {datetime.now().strftime('%d %b %y')}")
                    
                    st.markdown("---")
                    
                    def parse_card_values(raw_text: str) -> Tuple[str, str]:
                        """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å card-infos ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå"""
                        if not raw_text:
                            return "", ""

                        lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
                        values = {"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à": "", "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå": ""}
                        current_label: Optional[str] = None

                        for line in lines:
                            normalized = line.replace(':', '').strip()

                            if normalized in values:
                                current_label = normalized
                                value = ""
                                if ':' in line:
                                    value = line.split(':', 1)[1].strip()

                                if value:
                                    values[normalized] = value
                                    current_label = None
                                else:
                                    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                                    values.setdefault(normalized, "")
                                continue

                            if current_label:
                                if values[current_label]:
                                    values[current_label] += f" {line}"
                                else:
                                    values[current_label] = line

                        return values.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à", ""), values.get("‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå", "")

                    # === ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà 1: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ===
                    st.subheader("üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó")
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                    info_items = []
                    info_values = []
                    
                    # 1. ‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
                    if company_info.get("company_name"):
                        info_items.append("‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                        info_values.append(str(company_info.get("company_name", "-")))
                    
                    # 2. ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
                    if company_info.get("registration_number"):
                        info_items.append("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                        info_values.append(str(company_info.get("registration_number", "-")))
                    
                    # 3. ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏´‡πà‡∏á‡πÉ‡∏´‡∏ç‡πà
                    if company_info.get("address"):
                        info_items.append("‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏´‡πà‡∏á‡πÉ‡∏´‡∏ç‡πà")
                        address = str(company_info.get("address", "-"))
                        info_values.append(address.replace('\n', ' '))
                    
                    # 4. ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£ (‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
                    if company_info.get("directors"):
                        directors_text = company_info.get("directors", "").strip()
                        if directors_text:
                            directors_list = [d.strip() for d in directors_text.split('\n') if d.strip()]
                            # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                            filtered_directors = []
                            for director in directors_list:
                                if '‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£' not in director:
                                    filtered_directors.append(director)
                            
                            if filtered_directors:
                                directors_str = " ".join([f"{i+1}. {d}" for i, d in enumerate(filtered_directors)])
                                info_items.append("‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£")
                                info_values.append(directors_str)
                    
                    # 5. ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡∏Å‡∏û‡∏±‡∏ô
                    if company_info.get("authorized_signatories"):
                        auth_text = company_info.get("authorized_signatories", "").strip()
                        if auth_text:
                            if '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡∏Å‡∏û‡∏±‡∏ô' in auth_text:
                                auth_text = auth_text.replace('‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡∏Å‡∏û‡∏±‡∏ô', '').strip()
                            info_items.append("‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡∏Å‡∏û‡∏±‡∏ô")
                            info_values.append(auth_text)
                    
                    # 6. ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
                    if company_info.get("business_type"):
                        info_items.append("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                        info_values.append(str(company_info.get("business_type", "-")))
                    
                    # 7. ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•
                    if company_info.get("status"):
                        info_items.append("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                        status = str(company_info.get("status", "-"))
                        info_values.append(status)
                    
                    # 8. ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á
                    if company_info.get("found_date"):
                        info_items.append("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á")
                        info_values.append(str(company_info.get("found_date", "-")))
                    
                    # 9. ‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
                    if company_info.get("registered_capital"):
                        info_items.append("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                        info_values.append(str(company_info.get("registered_capital", "-")))
                    
                    # 10. ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏î‡∏¥‡∏°
                    if company_info.get("old_registration_number"):
                        info_items.append("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏î‡∏¥‡∏°")
                        old_reg = str(company_info.get("old_registration_number", "-"))
                        info_values.append(old_reg if old_reg != "-" else "-")
                    
                    # 11. ‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
                    if company_info.get("business_group"):
                        info_items.append("‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à")
                        info_values.append(str(company_info.get("business_group", "-")))
                    
                    # 12. ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
                    if company_info.get("business_size"):
                        info_items.append("‡∏Ç‡∏ô‡∏≤‡∏î‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à")
                        info_values.append(str(company_info.get("business_size", "-")))
                    
                    # 13. Website
                    if company_info.get("website"):
                        info_items.append("Website")
                        website = str(company_info.get("website", "-"))
                        if website.startswith('-'):
                            info_values.append(website)
                        elif website.startswith('http'):
                            info_values.append(f"- [{website}]({website})")
                        else:
                            info_values.append(f"- {website}")
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    if info_items:
                        info_data = {
                            "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£": info_items,
                            "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•": info_values
                        }
                        df_result = pd.DataFrame(info_data)
                        st.dataframe(df_result, use_container_width=True, hide_index=True)
                    else:
                        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó")
                    
                    st.markdown("---")
                    
                    # === ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ===
                    st.subheader("üè¢ ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                    
                    biz_reg_type = (company_info.get("business_type_registration") or "").strip()
                    biz_reg_objective = (company_info.get("business_type_registration_objective") or "").strip()

                    if not (biz_reg_type or biz_reg_objective):
                        raw_text = (company_info.get("business_type_registration_raw") or "").strip()
                        if raw_text:
                            parsed_type, parsed_objective = parse_card_values(raw_text)
                            biz_reg_type = parsed_type.strip()
                            biz_reg_objective = parsed_objective.strip()

                    if biz_reg_type or biz_reg_objective:
                        reg_data = {
                            "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à": [biz_reg_type if biz_reg_type else "-"],
                            "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå": [biz_reg_objective if biz_reg_objective else "-"]
                        }
                        df_biz_reg = pd.DataFrame(reg_data)
                        st.dataframe(df_biz_reg, use_container_width=True, hide_index=True)
                    else:
                        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡∏≠‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                    
                    st.markdown("---")
                    
                    # === ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà 3: ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏µ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ===
                    st.subheader("üìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏µ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
                    
                    biz_latest_type = (company_info.get("business_type_latest") or "").strip()
                    biz_latest_objective = (company_info.get("business_type_latest_objective") or "").strip()

                    if not (biz_latest_type or biz_latest_objective):
                        raw_latest_text = (company_info.get("business_type_latest_raw") or "").strip()
                        if raw_latest_text:
                            parsed_type, parsed_objective = parse_card_values(raw_latest_text)
                            biz_latest_type = parsed_type.strip()
                            biz_latest_objective = parsed_objective.strip()

                    if biz_latest_type or biz_latest_objective:
                        latest_data = {
                            "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à": [biz_latest_type if biz_latest_type else "-"],
                            "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå": [biz_latest_objective if biz_latest_objective else "-"]
                        }
                        df_biz_latest = pd.DataFrame(latest_data)
                        st.dataframe(df_biz_latest, use_container_width=True, hide_index=True)
                    else:
                        st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏µ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
                    
                    # === ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Expander) ===
                    if company_info.get("company_details"):
                        st.markdown("---")
                        with st.expander("üìÑ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡∏¢‡∏≤‡∏¢)", expanded=False):
                            # ‡πÅ‡∏¢‡∏Å‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà
                            details_text = company_info.get("company_details", "").strip()
                            if details_text:
                                lines = details_text.split('\n')
                                current_section = None
                                
                                for line in lines:
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å (‡πÑ‡∏°‡πà‡∏°‡∏µ ":")
                                    if '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•' in line and ':' not in line:
                                        current_section = "company_info"
                                        st.markdown(f"### {line}")
                                        continue
                                    elif '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à' in line and ':' not in line:
                                        if current_section:
                                            st.markdown("---")
                                        current_section = "business_group"
                                        st.markdown(f"### {line}")
                                        continue
                                    elif '‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô' in line and ':' not in line:
                                        if current_section:
                                            st.markdown("---")
                                        current_section = "financial_years"
                                        st.markdown(f"### {line}")
                                        continue
                                    elif '‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÅ‡∏´‡πà‡∏á‡πÉ‡∏´‡∏ç‡πà' in line and ':' not in line:
                                        if current_section:
                                            st.markdown("---")
                                        current_section = "address"
                                        st.markdown(f"### {line}")
                                        continue
                                    elif 'Website' in line and ':' not in line:
                                        if current_section:
                                            st.markdown("---")
                                        current_section = "website"
                                        st.markdown(f"### {line}")
                                        continue
                                    
                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Key-Value
                                    if ':' in line:
                                        parts = line.split(':', 1)
                                        if len(parts) == 2:
                                            key = parts[0].strip()
                                            value = parts[1].strip()
                                            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô URL ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô link
                                            if value.startswith('http') or '://' in value:
                                                st.markdown(f"**{key}:** - [{value}]({value})")
                                            else:
                                                st.markdown(f"**{key}:** {value}")
                                        else:
                                            st.write(line)
                                    else:
                                        # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡πÄ‡∏ä‡πà‡∏ô note)
                                        st.write(line)
                            else:
                                st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
    
    elif mode == "üìä ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel":
        st.subheader("üìä ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel")
        
        uploaded_excel_bot = st.file_uploader(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel",
            type=['xlsx', 'xls'],
            help="‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
            key="excel_upload_bot"
        )
        
        if uploaded_excel_bot is not None:
            try:
                # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel
                df_excel_bot = pd.read_excel(uploaded_excel_bot)
                
                st.success("‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                st.subheader("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
                st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß:** {len(df_excel_bot)}")
                st.write(f"**‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:** {len(df_excel_bot.columns)}")
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
                st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà:**")
                st.write(", ".join(df_excel_bot.columns.tolist()))
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
                company_columns = [col for col in df_excel_bot.columns if any(keyword in col.lower() for keyword in ['‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏ä‡∏∑‡πà‡∏≠', 'company', 'name'])]
                
                if company_columns:
                    selected_column_bot = st.selectbox(
                        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:",
                        company_columns,
                        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                        key="company_column_bot"
                    )
                    
                    if selected_column_bot:
                        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
                        st.subheader("üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
                        st.dataframe(df_excel_bot.head(10), use_container_width=True)
                        
                        # ‡∏õ‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
                        if st.button("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", type="primary", use_container_width=True, key="process_excel_bot"):
                            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ browser mode
                            if use_browser_mode and not headless_mode:
                                st.info("üëÄ **‡∏î‡∏π Chromium Browser ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà** - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå!")
                            
                            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD
                            df_excel_with_dbd = integrate_with_streamlit(
                                df_excel_bot.copy(), 
                                selected_column_bot,
                                use_browser=use_browser_mode,
                                headless=headless_mode
                            )
                            
                            # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
                            dbd_summary = create_dbd_summary_table(df_excel_with_dbd)
                            
                            if not dbd_summary.empty:
                                st.subheader("üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD")
                                st.dataframe(dbd_summary, use_container_width=True)
                            
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD
                            output_dbd = io.BytesIO()
                            with pd.ExcelWriter(output_dbd, engine='openpyxl') as writer:
                                df_excel_with_dbd.to_excel(writer, sheet_name='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD', index=False)
                                
                                if not dbd_summary.empty:
                                    dbd_summary.to_excel(writer, sheet_name='‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD', index=False)
                            
                            output_dbd.seek(0)
                            
                            st.download_button(
                                label="üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD",
                                data=output_dbd.getvalue(),
                                file_name=f"excel_with_dbd_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                help="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Excel ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD DataWarehouse",
                                key="download_dbd_bot"
                            )
                            
                            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
                            st.subheader("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß")
                            st.dataframe(df_excel_with_dbd, use_container_width=True)
                else:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                    st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà:**")
                    for col in df_excel_bot.columns:
                        st.write(f"‚Ä¢ {col}")
            
            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel: {str(e)}")

def render_receipt_bot_page():
    """‡∏´‡∏ô‡πâ‡∏≤ Bot ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à - ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö ‡πÅ‡∏Ñ‡πà‡∏õ‡∏∏‡πà‡∏°‡πÑ‡∏ß‡πâ"""
    st.header("üßæ Bot ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à")
    st.markdown("---")
    st.write("**‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**")
    st.info("üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å DBD ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏Å‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö PEAKEngine")

    uploaded_peak_excel = st.file_uploader(
        "üìÅ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD'",
        type=['xlsx', 'xls'],
        help="‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD' ‡πÅ‡∏•‡∏∞ '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
    )

    def normalize_registration_number(reg_value):
        if pd.isna(reg_value):
            return ""
        reg_str = str(reg_value).strip()
        if not reg_str:
            return ""
        digits = "".join(ch for ch in reg_str if ch.isdigit())
        if not digits:
            return ""
        digits = digits[-13:]
        if len(digits) < 13:
            digits = digits.zfill(13)
        if digits[0] != "0":
            digits = "0" + digits[1:]
        return digits

    def parse_dbd_info(text: str) -> Dict[str, str]:
        results = {}
        if not text or pd.isna(text):
            return results
        parts = [part.strip() for part in str(text).split('|')]
        for part in parts:
            if ':' not in part:
                continue
            key, value = part.split(':', 1)
            results[key.strip()] = value.strip()
        return results

    def slugify_filename(value: Any) -> str:
        text = str(value).strip()
        if not text or text.lower() in {"nan", "none"}:
            return "type"
        sanitized = re.sub(r"[^\w‡∏Å-‡πô\-]+", "_", text)
        sanitized = re.sub(r"_+", "_", sanitized).strip("_")
        return sanitized or "type"

    def build_summary_from_source(source_df: pd.DataFrame) -> pd.DataFrame:
        if source_df is None or source_df.empty:
            return pd.DataFrame()

        summary_rows: List[Dict[str, Any]] = []
        for _, row in source_df.iterrows():
            dbd_parsed = parse_dbd_info(row.get("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", ""))
            reg_candidates = [
                row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"),
                row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"),
                row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"),
                dbd_parsed.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
            ]
            normalized_reg = ""
            for candidate in reg_candidates:
                normalized_reg = normalize_registration_number(candidate)
                if normalized_reg:
                    break
            if not normalized_reg:
                continue

            summary_row: Dict[str, Any] = {}
            columns_to_preserve = [
                "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD",
                "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD",
                "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô",
                "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD",
                "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD",
                "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à",
                "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£",
                "‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
                "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå"
            ]
            for column_name in columns_to_preserve:
                if column_name in row:
                    summary_row[column_name] = row.get(column_name, "")

            summary_row["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"] = normalized_reg
            summary_row["‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏Å DBD"] = summary_row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à", "") or dbd_parsed.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à", "")
            summary_row["‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏≤‡∏Å DBD"] = summary_row.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£", "") or dbd_parsed.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", "")
            summary_row["‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"] = summary_row.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "") or dbd_parsed.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "")
            summary_row["‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏≤‡∏Å DBD"] = summary_row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà", "") or dbd_parsed.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà", "")

            summary_rows.append(summary_row)

        if not summary_rows:
            return pd.DataFrame()

        summary_df = pd.DataFrame(summary_rows)
        return summary_df

    if uploaded_peak_excel is not None:
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå Excel..."):
            try:
                excel_file = pd.ExcelFile(uploaded_peak_excel)
                if "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD" not in excel_file.sheet_names:
                    available_sheets = ", ".join(excel_file.sheet_names)
                    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î")
                    if available_sheets:
                        st.info(f"üìÑ ‡∏ä‡∏µ‡∏ï‡∏ó‡∏µ‡πà‡∏û‡∏ö: {available_sheets}")
                else:
                    df_peak = pd.read_excel(excel_file, sheet_name="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD")
                    st.success("‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

                    available_company_cols = [col for col in df_peak.columns if "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó" in str(col)]
                    transfer_type_col = next((col for col in df_peak.columns if "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô" in str(col)), None)

                    df_peak_filtered = df_peak.copy()
                    selected_transfer_types_state: List[str] = st.session_state.get("peakengine_selected_transfer_types", [])

                    if transfer_type_col:
                        st.subheader("üéØ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
                        transfer_series = df_peak[transfer_type_col].astype(str).str.strip()
                        transfer_series = transfer_series[~transfer_series.str.lower().eq("nan")]
                        unique_transfer_types = sorted([value for value in transfer_series.unique().tolist() if value])

                        if unique_transfer_types:
                            default_selection = [value for value in selected_transfer_types_state if value in unique_transfer_types]
                            if not default_selection:
                                default_selection = unique_transfer_types

                            selected_transfer_types = st.multiselect(
                                "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:",
                                unique_transfer_types,
                                default=default_selection,
                                help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡πâ‡∏ô‡πÜ",
                                key="peakengine_transfer_type_multiselect"
                            )

                            st.session_state["peakengine_selected_transfer_types"] = selected_transfer_types

                            if selected_transfer_types:
                                filtered_mask = transfer_series.isin(selected_transfer_types)
                                df_peak_filtered = df_peak[filtered_mask].copy()
                                st.info(
                                    f"üìå ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å {len(selected_transfer_types)} ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô ‚Ä¢ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(df_peak_filtered)} ‡∏à‡∏≤‡∏Å {len(df_peak)} ‡πÅ‡∏ñ‡∏ß",
                                    icon="üìä"
                                )
                            else:
                                df_peak_filtered = df_peak.iloc[0:0].copy()
                                st.warning("‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                        else:
                            st.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ô‡∏µ‡πâ", icon="‚ÑπÔ∏è")
                            st.session_state["peakengine_selected_transfer_types"] = []
                    else:
                        st.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ ‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ", icon="‚ÑπÔ∏è")
                        st.session_state["peakengine_selected_transfer_types"] = []

                    st.session_state["peakengine_transfer_type_column"] = transfer_type_col

                    st.subheader("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", len(df_peak))
                    with col2:
                        st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", len(df_peak_filtered))
                    with col3:
                        st.metric("‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏ó‡∏µ‡πà‡∏û‡∏ö", len(available_company_cols))

                    st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï:**")
                    st.write(", ".join(df_peak.columns.astype(str).tolist()) or "-")

                    st.subheader("üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)")
                    if df_peak_filtered.empty:
                        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô")
                    else:
                        st.dataframe(df_peak_filtered.head(5), use_container_width=True)

                    if available_company_cols:
                        st.caption(f"üéØ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {', '.join(available_company_cols)}")
                    else:
                        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó' ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå")

                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ß‡πâ‡πÉ‡∏ô session state ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
                    def classify_dbd_status(value: Any) -> str:
                        if value is None:
                            return "‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á"
                        if isinstance(value, float) and pd.isna(value):
                            return "‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á"
                        text = str(value).strip()
                        lower_text = text.lower()
                        if not text or lower_text in {"nan", "none", ""}:
                            return "‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á"
                        if "‡πÑ‡∏°‡πà‡∏û‡∏ö" in lower_text or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text:
                            return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
                        if "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î" in lower_text or "error" in lower_text:
                            return "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"
                        if "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text or "‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text:
                            return "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
                        return "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

                    if not df_peak_filtered.empty:
                        df_peak_filtered = df_peak_filtered.copy()
                        df_peak_filtered.loc[:, "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"] = df_peak_filtered["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"].apply(classify_dbd_status)
                        status_counts = df_peak_filtered["‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"].value_counts()
                        st.subheader("üìä ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD")
                        col_status = st.columns(len(status_counts) if status_counts.size else 1)
                        for idx, (status_label, count_value) in enumerate(status_counts.items()):
                            with col_status[idx]:
                                st.metric(status_label, count_value)

                        with st.expander("üóÇÔ∏è ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", expanded=False):
                            available_statuses = sorted(df_peak_filtered["‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"].unique().tolist())
                            default_status_filter = st.session_state.get("peakengine_status_filter", available_statuses)
                            default_status_filter = [status for status in default_status_filter if status in available_statuses]
                            if not default_status_filter:
                                default_status_filter = available_statuses

                            status_filter = st.multiselect(
                                "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á",
                                options=available_statuses,
                                default=default_status_filter,
                                help="‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD'",
                                key="peakengine_status_filter"
                            )

                            search_keyword = st.text_input(
                                "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç",
                                value=st.session_state.get("peakengine_status_search", ""),
                                help="‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô‡πÜ",
                                key="peakengine_status_search"
                            )

                            max_rows = st.number_input(
                                "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á",
                                min_value=10,
                                max_value=max(10, len(df_peak_filtered)),
                                value=min(100, max(10, len(df_peak_filtered))),
                                step=10,
                                help="‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"
                            )

                            filtered_view = df_peak_filtered.copy()
                            if status_filter:
                                filtered_view = filtered_view[filtered_view["‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"].isin(status_filter)]

                            if search_keyword:
                                keyword = search_keyword.strip().lower()
                                if keyword:
                                    mask = (
                                        filtered_view["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"].astype(str).str.lower().str.contains(keyword, na=False)
                                        | filtered_view["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD"].astype(str).str.lower().str.contains(keyword, na=False)
                                        | filtered_view["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"].astype(str).str.lower().str.contains(keyword, na=False)
                                    )
                                    filtered_view = filtered_view[mask]

                            st.write(f"‡∏û‡∏ö {len(filtered_view)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(df_peak_filtered)})")

                            if filtered_view.empty:
                                st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
                            else:
                                st.dataframe(
                                    filtered_view.head(int(max_rows)),
                                    use_container_width=True
                                )

                    st.session_state["peakengine_source_df_full"] = df_peak
                    st.session_state["peakengine_source_df"] = df_peak_filtered
                    st.session_state["peakengine_source_filename"] = getattr(uploaded_peak_excel, "name", "uploaded.xlsx")

                    if transfer_type_col and not df_peak.empty:
                        st.subheader("üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD)")
                        st.caption("‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD' ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)")

                        transfer_series_full = df_peak[transfer_type_col].astype(str).str.strip()
                        transfer_series_full = transfer_series_full[~transfer_series_full.str.lower().eq("nan")]
                        available_export_types = sorted([value for value in transfer_series_full.unique().tolist() if value])

                        if available_export_types:
                            export_datasets_step1: List[Tuple[str, bytes, str]] = []
                            timestamp_step1 = datetime.now().strftime("%Y%m%d_%H%M%S")
                            zip_buffer_step1 = io.BytesIO()

                            with zipfile.ZipFile(zip_buffer_step1, "w", zipfile.ZIP_DEFLATED) as zip_file:
                                for type_value in available_export_types:
                                    type_mask_full = transfer_series_full == type_value
                                    type_df_full = df_peak[type_mask_full].copy()
                                    if type_df_full.empty:
                                        continue

                                    excel_buffer = io.BytesIO()
                                    with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                                        type_df_full.to_excel(writer, sheet_name="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD", index=False)
                                    excel_buffer.seek(0)

                                    safe_type_name = slugify_filename(type_value)
                                    excel_filename = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°DBD_{safe_type_name}_{timestamp_step1}.xlsx"

                                    zip_file.writestr(excel_filename, excel_buffer.getvalue())
                                    export_datasets_step1.append((type_value, excel_buffer.getvalue(), excel_filename))

                            if export_datasets_step1:
                                zip_buffer_step1.seek(0)
                                st.download_button(
                                    label="üì¶ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏õ‡πá‡∏ô ZIP (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD)",
                                    data=zip_buffer_step1.getvalue(),
                                    file_name=f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°DBD_‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó_{timestamp_step1}.zip",
                                    mime="application/zip",
                                    key="download_step1_zip_all"
                                )

                                st.caption("‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à:")
                                num_cols_step1 = min(3, len(export_datasets_step1))
                                cols_step1 = st.columns(num_cols_step1)

                                for idx_export, (type_value, excel_bytes, excel_filename) in enumerate(export_datasets_step1):
                                    target_col = cols_step1[idx_export % num_cols_step1]
                                    with target_col:
                                        st.download_button(
                                            label=f"üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {type_value}",
                                            data=excel_bytes,
                                            file_name=excel_filename,
                                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                            key=f"download_step1_{slugify_filename(f'{type_value}_{idx_export}')}"
                                        )
                        else:
                            st.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î", icon="‚ÑπÔ∏è")

                    reg_info_map: Dict[str, Dict[str, Any]] = {}
                    for idx_row, row in df_peak_filtered.iterrows():
                        dbd_raw = row.get("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", "")
                        dbd_parsed = parse_dbd_info(dbd_raw)
                        reg_candidate = (
                            row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                            or row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD")
                            or row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                            or dbd_parsed.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                        )
                        reg_normalized = normalize_registration_number(reg_candidate)
                        if not reg_normalized:
                            continue
                        row_dict = row.to_dict()
                        reg_info_map[reg_normalized] = {
                            "registration": reg_normalized,
                            "dbd_raw": dbd_raw,
                            "dbd_info": dbd_parsed,
                            "transfer_type": str(row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô", "")).strip(),
                            "dbd_status": row.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", ""),
                            "company_name": row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD", ""),
                            "row_index": int(idx_row),
                            "row": row_dict
                        }
                    st.session_state["peakengine_reg_info_map"] = reg_info_map

                    summary_df: Optional[pd.DataFrame] = None
                    summary_title = "üìë ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"
                    summary_caption = ""

                    if "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD" in excel_file.sheet_names:
                        try:
                            df_summary = pd.read_excel(excel_file, sheet_name="‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD")

                            if "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô" in df_summary.columns:
                                df_summary["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"] = df_summary["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"].apply(normalize_registration_number)
                            else:
                                df_summary["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"] = ""

                            selected_transfer_types_summary = st.session_state.get("peakengine_selected_transfer_types", [])
                            if transfer_type_col and selected_transfer_types_summary and transfer_type_col in df_summary.columns:
                                summary_type_series = df_summary[transfer_type_col].astype(str).str.strip()
                                summary_mask = summary_type_series.isin(selected_transfer_types_summary)
                                df_summary = df_summary[summary_mask].copy()
                                if df_summary.empty:
                                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD' ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")

                            summary_df = df_summary if not df_summary.empty else None
                        except Exception as summary_error:
                            st.warning(f"‚ö†Ô∏è ‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡∏µ‡∏ï '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD' ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {summary_error}")

                    if summary_df is None or summary_df.empty:
                        summary_df = build_summary_from_source(df_peak_filtered)
                        summary_title = "üìë ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD"
                        summary_caption = "‚ö†Ô∏è ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ä‡∏µ‡∏ï '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° DBD' ‡πÅ‡∏ó‡∏ô ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ä‡∏µ‡∏ï '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD'"

                    if summary_df is not None and not summary_df.empty:
                        st.session_state["peakengine_summary_df"] = summary_df
                        st.subheader(summary_title)
                        if summary_caption:
                            st.caption(summary_caption)

                        reg_series = summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"))
                        valid_reg = []
                        if reg_series is not None:
                            reg_series = reg_series.astype(str).str.strip()
                            valid_reg = [reg for reg in reg_series if reg and reg.lower() != "nan"]

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô‡∏™‡∏£‡∏∏‡∏õ", len(summary_df))
                        with col2:
                            st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á", len(valid_reg))
                        with col3:
                            unique_regs = list(dict.fromkeys(valid_reg))
                            st.metric("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥", len(unique_regs))

                        st.write("**‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ:**")
                        st.write(", ".join(summary_df.columns.astype(str).tolist()) or "-")

                        st.subheader("üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ (5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)")
                        st.dataframe(summary_df.head(5), use_container_width=True)
                    else:
                        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                        st.session_state["peakengine_summary_df"] = pd.DataFrame()

            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel: {str(e)}")
    else:
        st.info("üì• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

    summary_df = st.session_state.get("peakengine_summary_df")
    if summary_df is not None and not summary_df.empty:
        st.markdown("---")
        st.subheader("üßæ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô PEAKEngine")

        reg_series = summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"))
        if reg_series is None:
            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå '‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô' ‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD'")
            return

        reg_series = reg_series.astype(str).str.strip()
        registration_numbers = [reg for reg in reg_series if reg and reg.lower() != "nan"]
        unique_registration_numbers = list(dict.fromkeys(registration_numbers))

        df_source_for_filters = st.session_state.get("peakengine_source_df", pd.DataFrame())

        def normalize_status_value(raw_status: Any) -> str:
            text = str(raw_status).strip()
            lower_text = text.lower()
            if not text or lower_text in {"nan", "none", ""}:
                return "‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á"
            if "‡πÑ‡∏°‡πà‡∏û‡∏ö" in lower_text or "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text:
                return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
            if "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î" in lower_text or "error" in lower_text:
                return "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"
            if "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text or "‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" in lower_text:
                return "‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
            return text

        def normalize_transfer_type(raw_type: Any) -> str:
            text = str(raw_type).strip()
            lower_text = text.lower()
            if not text or lower_text in {"nan", "none", "-", ""}:
                return "-"
            replacements = {
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó(‡∏ö‡∏à‡∏Å.)": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó(‡∏ö‡∏à‡∏Å)": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å)": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏≥‡∏Å‡∏±‡∏î": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)",
                "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•": "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                "‡∏´‡∏à‡∏Å.": "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)",
                "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î": "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)",
                "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô": "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)"
            }
            normalized = replacements.get(text, replacements.get(lower_text, text))
            return normalized

        def format_amount_display(raw_amount: Any, fallback_numeric: Optional[float]) -> str:
            if isinstance(raw_amount, (int, float)) and not pd.isna(raw_amount):
                return f"{float(raw_amount):,.2f}"
            if isinstance(raw_amount, str):
                amount_str = raw_amount.strip()
                if amount_str.lower() in {"nan", "none", ""}:
                    pass
                else:
                    return amount_str
            if fallback_numeric is not None:
                return f"{fallback_numeric:,.2f}"
            return "-"

        def format_date_display(raw_date: Any) -> str:
            if raw_date is None:
                return "-"
            if isinstance(raw_date, pd.Timestamp):
                try:
                    return raw_date.to_pydatetime().strftime("%d/%m/%Y")
                except Exception:
                    return raw_date.strftime("%d/%m/%Y")
            if isinstance(raw_date, datetime):
                return raw_date.strftime("%d/%m/%Y")
            if isinstance(raw_date, (int, float)):
                try:
                    if pd.isna(raw_date):
                        return "-"
                except Exception:
                    pass
                try:
                    base_date = datetime(1899, 12, 30)
                    converted = base_date + timedelta(days=float(raw_date))
                    return converted.strftime("%d/%m/%Y")
                except Exception:
                    pass
            text = str(raw_date).strip()
            if not text or text.lower() in {"nan", "none", "-", "--"}:
                return "-"
            normalized_text = text.replace("T", " ").split("+", 1)[0].strip()
            date_formats = [
                "%d/%m/%Y",
                "%d-%m-%Y",
                "%Y-%m-%d",
                "%Y/%m/%d",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M",
                "%d/%m/%Y %H:%M:%S",
                "%d/%m/%Y %H:%M",
                "%d-%m-%Y %H:%M:%S",
                "%d-%m-%Y %H:%M",
                "%d %b %Y",
                "%d %b %y"
            ]
            for fmt in date_formats:
                try:
                    dt = datetime.strptime(normalized_text, fmt)
                    return dt.strftime("%d/%m/%Y")
                except Exception:
                    continue
            if " " in normalized_text:
                primary = normalized_text.split(" ", 1)[0].strip()
                if primary and primary != normalized_text:
                    formatted_primary = format_date_display(primary)
                    if formatted_primary and formatted_primary != "-":
                        return formatted_primary
            return normalized_text

        def get_row_amount(row_data: pd.Series) -> Optional[float]:
            amount_value = row_data.get("‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric")
            if amount_value is not None and not (isinstance(amount_value, float) and pd.isna(amount_value)):
                try:
                    return float(amount_value)
                except (TypeError, ValueError):
                    pass

            raw_amount = row_data.get("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô")
            if raw_amount is None:
                return None

            text_amount = str(raw_amount).strip()
            if not text_amount or text_amount.lower() in {"nan", "none"}:
                return None

            negative = False
            if text_amount.startswith("(") and text_amount.endswith(")"):
                negative = True
                text_amount = text_amount[1:-1]

            text_amount = text_amount.replace(",", "").replace("+", "").strip()
            if not text_amount:
                return None

            try:
                value = float(text_amount)
                return -value if negative else value
            except ValueError:
                return None

        def normalize_company_key(raw_name: Any) -> str:
            if raw_name is None:
                return ""
            text = str(raw_name).strip().lower()
            if not text or text in {"nan", "none", ""}:
                return ""
            replacements = [
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≥‡∏Å‡∏±‡∏î", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó ‡∏à‡∏≥‡∏Å‡∏±‡∏î", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó(‡∏à‡∏≥‡∏Å‡∏±‡∏î)", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏à‡∏≥‡∏Å‡∏±‡∏î)",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó(‡∏ö‡∏à‡∏Å.)", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≥‡∏Å‡∏±‡∏î (‡∏°‡∏´‡∏≤‡∏ä‡∏ô)", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î",
                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡∏ö‡∏à‡∏Å.", "‡∏ö‡∏à‡∏Å", "‡∏à‡∏≥‡∏Å‡∏±‡∏î", "‡∏°‡∏´‡∏≤‡∏ä‡∏ô",
                "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î", "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô", "‡∏´‡∏à‡∏Å.", "‡∏´‡∏à‡∏Å"
            ]
            for phrase in replacements:
                text = text.replace(phrase.lower(), "")
            text = re.sub(r"[^\w‡∏Å-‡πô]+", "", text)
            return text

        def clean_text(value: Any) -> str:
            if value is None:
                return ""
            text = str(value).strip()
            if not text or text.lower() in {"nan", "none"}:
                return ""
            return text

        def pick_first_text(*values: Any) -> str:
            for value in values:
                text = clean_text(value)
                if text:
                    return text
            return ""

        summary_df_for_lookup = st.session_state.get("peakengine_summary_df")
        summary_lookup: Dict[str, Dict[str, Any]] = {}
        if summary_df_for_lookup is not None and not summary_df_for_lookup.empty:
            for _, summary_row in summary_df_for_lookup.iterrows():
                reg_candidates = [
                    summary_row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"),
                    summary_row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                ]
                normalized_reg = ""
                for candidate in reg_candidates:
                    normalized_reg = normalize_registration_number(candidate)
                    if normalized_reg:
                        break
                if not normalized_reg:
                    continue
                summary_data_dict = summary_row.to_dict()
                potential_names = []
                for col_name in [
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á)",
                    "CompanyName"
                ]:
                    if col_name in summary_row and pd.notna(summary_row[col_name]):
                        potential_names.append(summary_row[col_name])
                for name in potential_names:
                    key = normalize_company_key(name)
                    if key and key not in summary_lookup:
                        summary_lookup[key] = {
                            "registration": normalized_reg,
                            "data": summary_data_dict
                        }

        positive_rows: List[Dict[str, Any]] = []
        if df_source_for_filters is not None and not df_source_for_filters.empty:
            for idx, row in df_source_for_filters.iterrows():
                amount_numeric = get_row_amount(row)
                if amount_numeric is None or amount_numeric <= 0:
                    continue

                dbd_info_raw_value = clean_text(row.get("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", "")) or str(row.get("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", "")).strip()
                dbd_info_dict = parse_dbd_info(dbd_info_raw_value)

                reg_candidate = (
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                    or row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD")
                    or row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                    or dbd_info_dict.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                )
                reg_norm = normalize_registration_number(reg_candidate)

                status_value = normalize_status_value(row.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD", ""))

                transfer_type_value = normalize_transfer_type(row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô", ""))

                company_value_raw = (
                    row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD")
                    or row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                    or row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó")
                    or ""
                )
                company_value = str(company_value_raw).strip() or "-"

                work_category_raw = row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                work_category_value = clean_text(work_category_raw) or "-"
                normalized_work_category = work_category_value.strip().lower()
                skip_work_categories = {"", "-", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏á‡∏≤‡∏ô", "‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏¥‡∏•‡πÅ‡∏•‡πâ‡∏ß", "‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏¥‡∏•‡πÄ‡∏≠‡∏á", "‡∏ö‡∏≠‡∏ó‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"}
                valid_work_categories = {"‡∏†‡∏≤‡∏©‡∏µ‡∏õ‡∏Å‡∏ï‡∏¥", "‡∏´‡∏±‡∏Å ‡∏ì ‡∏ó‡∏µ‡πà‡∏à‡πà‡∏≤‡∏¢"}

                if normalized_work_category in skip_work_categories:
                    continue
                if normalized_work_category and normalized_work_category not in valid_work_categories:
                    st.warning(
                        f"‚ö†Ô∏è ‡∏û‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: '{work_category_value}' (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {idx + 2}) ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°",
                        icon="‚ö†Ô∏è"
                    )
                    continue

                raw_date_value = row.get("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà")
                date_value = format_date_display(raw_date_value)

                amount_display = format_amount_display(row.get("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô"), amount_numeric)

                description_value_raw = str(row.get("‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢", "")).strip()
                description_value = description_value_raw if description_value_raw.lower() not in {"nan", "none", ""} else "-"

                # DBD detail fields
                dbd_company_name = pick_first_text(
                    row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD"),
                    dbd_info_dict.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó"),
                    dbd_info_dict.get("‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£")
                )
                dbd_registration_text = pick_first_text(
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"),
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"),
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"),
                    dbd_info_dict.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                )
                dbd_business_type = pick_first_text(
                    row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏Å DBD"),
                    row.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"),
                    dbd_info_dict.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à")
                )
                dbd_status_detail = pick_first_text(
                    row.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å DBD"),
                    row.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£"),
                    dbd_info_dict.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞")
                )
                dbd_capital = pick_first_text(
                    row.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"),
                    row.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"),
                    dbd_info_dict.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                )
                dbd_address_text = pick_first_text(
                    row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏≤‡∏Å DBD"),
                    dbd_info_dict.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà")
                )
                dbd_address_house_no = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà"))
                dbd_address_village = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô"))
                dbd_address_moo = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà"))
                dbd_address_subdistrict = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•"))
                dbd_address_district = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠"))
                dbd_address_province = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î"))
                dbd_address_postal_code = clean_text(row.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå"))

                summary_match: Optional[Dict[str, Any]] = None
                if transfer_type_value in {"‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏ö‡∏à‡∏Å.)", "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô (‡∏´‡∏à‡∏Å.)"} and summary_lookup:
                    candidate_names = [
                        row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD", ""),
                        row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", ""),
                        row.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", ""),
                        company_value,
                        description_value
                    ]
                    for candidate_name in candidate_names:
                        key = normalize_company_key(candidate_name)
                        if key and key in summary_lookup:
                            summary_match = summary_lookup[key]
                            break

                if summary_match:
                    summary_data = summary_match["data"]
                    if not reg_norm:
                        reg_from_summary = summary_match.get("registration")
                        reg_norm = normalize_registration_number(reg_from_summary) or reg_norm

                    dbd_company_name = dbd_company_name or pick_first_text(
                        summary_data.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD"),
                        summary_data.get("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó")
                    )
                    dbd_registration_text = dbd_registration_text or pick_first_text(
                        summary_data.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"),
                        summary_data.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô")
                    )
                    dbd_business_type = dbd_business_type or pick_first_text(summary_data.get("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"))
                    dbd_status_detail = dbd_status_detail or pick_first_text(summary_data.get("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞"))
                    dbd_capital = dbd_capital or pick_first_text(summary_data.get("‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"))
                    dbd_address_text = dbd_address_text or pick_first_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà"))
                    dbd_address_house_no = dbd_address_house_no or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà"))
                    dbd_address_village = dbd_address_village or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô"))
                    dbd_address_moo = dbd_address_moo or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà"))
                    dbd_address_subdistrict = dbd_address_subdistrict or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•"))
                    dbd_address_district = dbd_address_district or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠"))
                    dbd_address_province = dbd_address_province or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î"))
                    dbd_address_postal_code = dbd_address_postal_code or clean_text(summary_data.get("‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå"))

                dbd_registration_numeric = normalize_registration_number(dbd_registration_text)
                if not reg_norm:
                    reg_norm = dbd_registration_numeric or reg_norm
                if not reg_norm:
                    continue

                dbd_registration_display = pick_first_text(
                    dbd_registration_text,
                    dbd_info_dict.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"),
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"),
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"),
                    row.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•")
                ) or "-"

                row_unique_key = f"{idx}_{len(positive_rows)}"
                positive_rows.append({
                    "row_key": row_unique_key,
                    "registration": reg_norm,
                    "dbd_status": status_value,
                    "transfer_type": transfer_type_value or "-",
                    "company_name": company_value or "-",
                    "document_date_raw": raw_date_value,
                    "work_category": work_category_value,
                    "date": date_value,
                    "amount": amount_display,
                    "amount_numeric": amount_numeric,
                    "description": description_value,
                    "dbd_info_raw": dbd_info_raw_value or "-",
                    "dbd_company_name": dbd_company_name or "-",
                    "dbd_registration": dbd_registration_display,
                    "dbd_business_type": dbd_business_type or "-",
                    "dbd_status_detail": dbd_status_detail or "-",
                    "dbd_capital": dbd_capital or "-",
                    "dbd_address_text": dbd_address_text or "-",
                    "dbd_address_house_no": dbd_address_house_no or "-",
                    "dbd_address_village": dbd_address_village or "-",
                    "dbd_address_moo": dbd_address_moo or "-",
                    "dbd_address_subdistrict": dbd_address_subdistrict or "-",
                    "dbd_address_district": dbd_address_district or "-",
                    "dbd_address_province": dbd_address_province or "-",
                    "dbd_address_postal_code": dbd_address_postal_code or "-",
                    "source_index": idx
                })

        if not positive_rows:
            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ß‡∏Å‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2")
            return

        df_positive_rows = pd.DataFrame(positive_rows)
        total_positive = len(df_positive_rows)
        unique_positive = df_positive_rows["registration"].nunique()

        status_counts = df_positive_rows["dbd_status"].value_counts()
        type_counts = df_positive_rows["transfer_type"].value_counts()

        st.caption("‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á")
        col_summary_left, col_summary_right = st.columns(2)
        with col_summary_left:
            st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß (‡∏¢‡∏≠‡∏î‡∏ö‡∏ß‡∏Å)", total_positive)
            st.metric("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥", unique_positive)
        with col_summary_right:
            st.write("**‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD**")
            status_display_df = status_counts.rename_axis("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞").reset_index(name="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")
            st.dataframe(status_display_df, height=150, hide_index=True)
        st.write("**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô**")
        type_display_df = type_counts.rename_axis("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó").reset_index(name="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")
        st.dataframe(type_display_df, height=150, hide_index=True)

        all_possible_statuses = ["‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á", "‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"]
        available_step2_statuses = sorted(
            {status for status in df_positive_rows["dbd_status"].dropna().unique().tolist() if status} |
            set(all_possible_statuses)
        )

        available_step2_types = sorted(
            {type_value for type_value in df_positive_rows["transfer_type"].dropna().unique().tolist() if type_value and type_value != "-"}
        )
        if df_positive_rows["transfer_type"].isin(["-"]).any() and "-" not in available_step2_types:
            available_step2_types.insert(0, "-")

        st.subheader("‚öôÔ∏è ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2")

        step1_status_pref = st.session_state.get("peakengine_status_filter", [])
        normalized_step1_pref = [
            status for status in step1_status_pref if status in available_step2_statuses
        ]
        snapshot_key = "peakengine_step1_status_filter_snapshot"
        step1_snapshot = tuple(sorted(normalized_step1_pref))
        previous_snapshot = st.session_state.get(snapshot_key)
        if step1_snapshot != previous_snapshot:
            st.session_state[snapshot_key] = step1_snapshot
            st.session_state["peakengine_step2_status_filter"] = normalized_step1_pref or available_step2_statuses
        elif "peakengine_step2_status_filter" not in st.session_state:
            st.session_state["peakengine_step2_status_filter"] = normalized_step1_pref or available_step2_statuses

        default_step2_status_filter = st.session_state.get("peakengine_step2_status_filter", available_step2_statuses)
        default_step2_status_filter = [status for status in default_step2_status_filter if status in available_step2_statuses]
        if not default_step2_status_filter:
            default_step2_status_filter = available_step2_statuses

        pending_status_filter = st.session_state.pop("peakengine_step2_status_filter_pending", None)
        if pending_status_filter is not None:
            st.session_state["peakengine_step2_status_filter"] = [status for status in pending_status_filter if status in available_step2_statuses]
            st.experimental_rerun()

        selected_step2_statuses = st.multiselect(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏Å:",
            options=available_step2_statuses,
            default=default_step2_status_filter,
            help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
            key="peakengine_step2_status_filter"
        )

        col_status_btn1, col_status_btn2 = st.columns(2)
        with col_status_btn1:
            if st.button("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", key="step2_status_select_all"):
                st.session_state["peakengine_step2_status_filter_pending"] = available_step2_statuses
        with col_status_btn2:
            if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", key="step2_status_clear"):
                st.session_state["peakengine_step2_status_filter_pending"] = []

        default_step2_type_filter = st.session_state.get("peakengine_step2_type_filter", available_step2_types)
        default_step2_type_filter = [type_value for type_value in default_step2_type_filter if type_value in available_step2_types]
        if not default_step2_type_filter and available_step2_types:
            default_step2_type_filter = available_step2_types

        pending_type_filter = st.session_state.pop("peakengine_step2_type_filter_pending", None)
        if pending_type_filter is not None:
            st.session_state["peakengine_step2_type_filter"] = [value for value in pending_type_filter if value in available_step2_types]
            st.experimental_rerun()

        selected_step2_types = st.multiselect(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏Å:",
            options=available_step2_types,
            default=default_step2_type_filter,
            help="‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏≠‡∏ó (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó)",
            key="peakengine_step2_type_filter"
        )

        if available_step2_types:
            col_type_btn1, col_type_btn2 = st.columns(2)
            with col_type_btn1:
                if st.button("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", key="step2_type_select_all"):
                    st.session_state["peakengine_step2_type_filter_pending"] = available_step2_types
            with col_type_btn2:
                if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó", key="step2_type_clear"):
                    st.session_state["peakengine_step2_type_filter_pending"] = []

        step2_search_keyword = st.text_input(
            "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á)",
            value=st.session_state.get("peakengine_step2_search", ""),
            key="peakengine_step2_search"
        ).strip()

        show_preview_step2 = st.checkbox(
            "‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á)",
            value=st.session_state.get("peakengine_step2_show_preview", False),
            key="peakengine_step2_show_preview"
        )

        filter_mask = pd.Series(True, index=df_positive_rows.index)
        if selected_step2_statuses:
            normalized_selected = [
                "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏°" if status == "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" else status
                for status in selected_step2_statuses
            ]
            filter_mask &= (
                df_positive_rows["dbd_status"].isin(selected_step2_statuses)
                & ~df_positive_rows["dbd_status"].astype(str).str.contains("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", na=False)
            )

        if selected_step2_types and available_step2_types:
            filter_mask &= df_positive_rows["transfer_type"].isin(selected_step2_types)

        if step2_search_keyword:
            keyword_lower = step2_search_keyword.lower()
            filter_mask &= (
                df_positive_rows["registration"].str.contains(keyword_lower, case=False, na=False)
                | df_positive_rows["company_name"].str.contains(keyword_lower, case=False, na=False)
                | df_positive_rows["description"].str.contains(keyword_lower, case=False, na=False)
            )

        df_step2_filtered = df_positive_rows[filter_mask].copy().reset_index(drop=True)

        if df_step2_filtered.empty:
            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2")
            return

        if "row_key" in df_step2_filtered.columns:
            df_step2_filtered["row_key"] = (
                df_step2_filtered["row_key"]
                .astype(str)
                .fillna("")
                .apply(lambda x: x.strip())
            )
            missing_row_mask = df_step2_filtered["row_key"].isin(["", "nan", "none", "None"])
            if missing_row_mask.any():
                for idx_masked, _ in df_step2_filtered[missing_row_mask].iterrows():
                    df_step2_filtered.at[idx_masked, "row_key"] = f"row_{idx_masked}"
        else:
            df_step2_filtered["row_key"] = [f"row_{idx}" for idx in range(len(df_step2_filtered))]

        raw_not_found_mask = df_step2_filtered.get("dbd_info_raw", df_step2_filtered["dbd_status"]).astype(str).str.contains("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", case=False, na=False)
        status_not_found_mask = df_step2_filtered["dbd_status"].astype(str).str.strip() == "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
        not_found_mask = raw_not_found_mask | status_not_found_mask

        company_display_series = df_step2_filtered["company_name"].astype(str).where(~not_found_mask, "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏°")
        status_display_series = df_step2_filtered["dbd_status"].astype(str).where(~not_found_mask, "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏°")

        df_step2_filtered["company_name_display"] = company_display_series
        df_step2_filtered["dbd_status_display"] = status_display_series

        filtered_total = len(df_step2_filtered)
        filtered_unique_total = df_step2_filtered["registration"].nunique()

        st.write(
            f"üìå ‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏¢‡∏≠‡∏î‡∏ö‡∏ß‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_positive} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ {unique_positive}) "
            f"‚Ä¢ ‡∏´‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {filtered_total} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ {filtered_unique_total})"
        )

        row_records = df_step2_filtered.to_dict("records")
        row_keys = [record.get("row_key") for record in row_records]
        registration_numbers = [record.get("registration") for record in row_records]
        unique_registration_numbers = list(dict.fromkeys(registration_numbers))
        row_key_map = {
            record.get("row_key"): record for record in row_records if record.get("row_key") is not None
        }

        not_found_df = df_step2_filtered[not_found_mask].copy()
        st.session_state["peakengine_step2_not_found_df"] = not_found_df
        st.session_state["peakengine_step2_not_found_regs"] = not_found_df["registration"].tolist()
        st.session_state["peakengine_step2_all_df"] = df_step2_filtered.copy()
        st.session_state["peakengine_step2_row_records"] = row_records
        st.session_state["peakengine_step2_row_key_map"] = row_key_map

        export_columns = {
            "registration": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
            "company_name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• (‡πÄ‡∏î‡∏¥‡∏°)",
            "company_name_display": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
            "work_category": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
            "date": "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
            "amount": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
            "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
            "dbd_status": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD (‡πÄ‡∏î‡∏¥‡∏°)",
            "dbd_status_display": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD",
            "transfer_type": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô",
            "dbd_company_name": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD",
            "dbd_registration": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD",
            "dbd_business_type": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à",
            "dbd_status_detail": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£",
            "dbd_capital": "‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
            "dbd_address_text": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)",
            "dbd_address_house_no": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà",
            "dbd_address_village": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
            "dbd_address_moo": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà",
            "dbd_address_subdistrict": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•",
            "dbd_address_district": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
            "dbd_address_province": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
            "dbd_address_postal_code": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå",
            "dbd_info_raw": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD (‡∏î‡∏¥‡∏ö)"
        }

        download_types = [
            value for value in df_step2_filtered["transfer_type"].dropna().unique().tolist()
            if str(value).strip()
        ]

        if download_types:
            st.subheader("üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô")
            st.caption("‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2 ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå")

            export_datasets: List[Tuple[str, bytes, str]] = []
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_buffer = io.BytesIO()

            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
                for type_value in sorted(download_types):
                    type_df = df_step2_filtered[df_step2_filtered["transfer_type"] == type_value]
                    if type_df.empty:
                        continue

                    export_df = type_df[list(export_columns.keys())].rename(columns=export_columns)

                    excel_buffer = io.BytesIO()
                    with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                        export_df.to_excel(writer, sheet_name="‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", index=False)
                    excel_buffer.seek(0)

                    safe_type = slugify_filename(type_value)
                    excel_filename = f"step2_{safe_type}_{timestamp_str}.xlsx"

                    zip_file.writestr(excel_filename, excel_buffer.getvalue())
                    export_datasets.append((type_value, excel_buffer.getvalue(), excel_filename))

            if export_datasets:
                zip_buffer.seek(0)
                st.download_button(
                    label="üì¶ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå ZIP",
                    data=zip_buffer.getvalue(),
                    file_name=f"step2_transfer_types_{timestamp_str}.zip",
                    mime="application/zip",
                    key="download_step2_zip_all"
                )

                st.caption("‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:")
                num_cols = min(3, len(export_datasets))
                cols = st.columns(num_cols)

                for idx, (type_value, excel_bytes, excel_filename) in enumerate(export_datasets):
                    target_col = cols[idx % num_cols]
                    with target_col:
                        st.download_button(
                            label=f"üì• ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {type_value}",
                            data=excel_bytes,
                            file_name=excel_filename,
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            key=f"download_step2_{slugify_filename(f'{type_value}_{idx}')}"
                        )

        if show_preview_step2:
            preview_columns = {
                "registration": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                "company_name_display": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                "work_category": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
                "date": "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                "amount": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
                "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
                "dbd_status_display": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD",
                "transfer_type": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô"
            }
            preview_df = df_step2_filtered[list(preview_columns.keys())].rename(columns=preview_columns)
            st.dataframe(
                preview_df.reset_index(drop=True),
                use_container_width=True,
                hide_index=True
            )

        st.session_state["peakengine_step2_filtered_df"] = df_step2_filtered

        existing_reg_info_map = st.session_state.get("peakengine_reg_info_map", {})
        reg_info_map_updated: Dict[str, Dict[str, Any]] = dict(existing_reg_info_map)
        row_payload_map: Dict[str, Dict[str, Any]] = {}

        if not df_source_for_filters.empty:
            for _, step2_row in df_step2_filtered.iterrows():
                reg_value = step2_row.get("registration")
                if not reg_value:
                    continue

                source_idx = step2_row.get("source_index")
                base_series = None
                if source_idx is not None and not (isinstance(source_idx, float) and pd.isna(source_idx)):
                    try:
                        base_series = df_source_for_filters.loc[source_idx]
                    except KeyError:
                        try:
                            base_series = df_source_for_filters.iloc[int(source_idx)]
                        except Exception:
                            base_series = None

                base_row_dict: Dict[str, Any] = {}
                if base_series is not None:
                    base_row_dict = base_series.to_dict()

                dbd_raw_candidates = [
                    base_row_dict.get("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"),
                    step2_row.get("dbd_info_raw"),
                    existing_reg_info_map.get(reg_value, {}).get("dbd_raw")
                ]
                dbd_raw_text = pick_first_text(*dbd_raw_candidates) or ""
                dbd_parsed = parse_dbd_info(dbd_raw_text)

                combined_row: Dict[str, Any] = {}
                combined_row.update(base_row_dict)

                step2_row_dict = step2_row.to_dict()
                for key, value in step2_row_dict.items():
                    if key not in combined_row or not combined_row[key]:
                        combined_row[key] = value

                # Map step2-specific address columns back to original column names if missing
                address_mapping = {
                    "dbd_address_house_no": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà",
                    "dbd_address_village": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
                    "dbd_address_moo": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà",
                    "dbd_address_subdistrict": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏ï‡∏≥‡∏ö‡∏•",
                    "dbd_address_district": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏≠‡∏≥‡πÄ‡∏†‡∏≠",
                    "dbd_address_province": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î",
                    "dbd_address_postal_code": "‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà_‡∏£‡∏´‡∏±‡∏™‡πÑ‡∏õ‡∏£‡∏©‡∏ì‡∏µ‡∏¢‡πå"
                }
                for source_key, target_key in address_mapping.items():
                    value = step2_row_dict.get(source_key)
                    if value and (target_key not in combined_row or not combined_row[target_key]):
                        combined_row[target_key] = value

                if dbd_raw_text and ("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD" not in combined_row or not combined_row["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"]):
                    combined_row["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"] = dbd_raw_text

                if step2_row_dict.get("company_name_display"):
                    combined_row.setdefault("‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•", step2_row_dict.get("company_name_display"))
                if step2_row_dict.get("work_category"):
                    combined_row.setdefault("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", step2_row_dict.get("work_category"))
                if step2_row_dict.get("document_date_raw") is not None:
                    if "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà" not in combined_row or not combined_row.get("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"):
                        combined_row["‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà"] = step2_row_dict.get("document_date_raw")
                if step2_row_dict.get("amount_numeric") is not None and (
                    "amount_numeric" not in combined_row or pd.isna(combined_row.get("amount_numeric"))
                ):
                    combined_row["amount_numeric"] = step2_row_dict.get("amount_numeric")
                if step2_row_dict.get("amount") and (
                    "amount" not in combined_row or not combined_row.get("amount")
                ):
                    combined_row["amount"] = step2_row_dict.get("amount")

                if step2_row_dict.get("dbd_company_name") and (
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD" not in combined_row or not combined_row["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD"]
                ):
                    combined_row["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD"] = step2_row_dict.get("dbd_company_name")

                if step2_row_dict.get("dbd_registration") and (
                    "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD" not in combined_row or not combined_row["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"]
                ):
                    combined_row["‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å DBD"] = step2_row_dict.get("dbd_registration")

                reg_entry = reg_info_map_updated.get(reg_value, {})
                reg_entry.update({
                    "registration": reg_value,
                    "transfer_type": step2_row_dict.get("transfer_type") or reg_entry.get("transfer_type", ""),
                    "dbd_status": step2_row_dict.get("dbd_status") or reg_entry.get("dbd_status", ""),
                    "dbd_status_display": step2_row_dict.get("dbd_status_display") or reg_entry.get("dbd_status_display", step2_row_dict.get("dbd_status", "")),
                    "company_name": step2_row_dict.get("company_name_display") or step2_row_dict.get("company_name") or reg_entry.get("company_name", ""),
                    "company_name_display": step2_row_dict.get("company_name_display") or reg_entry.get("company_name_display", step2_row_dict.get("company_name")),
                    "work_category": step2_row_dict.get("work_category") or reg_entry.get("work_category", ""),
                    "document_date_raw": step2_row_dict.get("document_date_raw") if step2_row_dict.get("document_date_raw") is not None else reg_entry.get("document_date_raw"),
                    "amount": step2_row_dict.get("amount") or reg_entry.get("amount", ""),
                    "amount_numeric": step2_row_dict.get("amount_numeric") if step2_row_dict.get("amount_numeric") is not None else reg_entry.get("amount_numeric"),
                    "dbd_raw": dbd_raw_text or reg_entry.get("dbd_raw", ""),
                    "dbd_info": dbd_parsed or reg_entry.get("dbd_info", {}),
                    "row_index": (
                        int(source_idx) if source_idx is not None and not (isinstance(source_idx, float) and pd.isna(source_idx))
                        else reg_entry.get("row_index")
                    ),
                    "row_key": step2_row_dict.get("row_key") or reg_entry.get("row_key"),
                    "row": combined_row,
                    "step2_row": step2_row_dict
                })
                reg_info_map_updated[reg_value] = reg_entry

                row_key_value = step2_row_dict.get("row_key")
                if row_key_value:
                    row_payload_map[row_key_value] = {
                        "registration": reg_value,
                        "transfer_type": step2_row_dict.get("transfer_type") or reg_entry.get("transfer_type", ""),
                        "dbd_status": step2_row_dict.get("dbd_status"),
                        "dbd_status_display": step2_row_dict.get("dbd_status_display"),
                        "company_name": reg_entry.get("company_name"),
                        "company_name_display": reg_entry.get("company_name_display"),
                        "work_category": step2_row_dict.get("work_category"),
                        "document_date_raw": step2_row_dict.get("document_date_raw"),
                        "amount": step2_row_dict.get("amount"),
                        "amount_numeric": step2_row_dict.get("amount_numeric"),
                        "dbd_raw": dbd_raw_text,
                        "dbd_info": dbd_parsed,
                        "row_index": (
                            int(source_idx) if source_idx is not None and not (isinstance(source_idx, float) and pd.isna(source_idx))
                            else reg_entry.get("row_index")
                        ),
                        "row": combined_row,
                        "step2_row": step2_row_dict
                    }

        st.session_state["peakengine_reg_info_map"] = reg_info_map_updated
        st.session_state["peakengine_row_payload_map"] = row_payload_map

        if not registration_numbers:
            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏Å")
            return

        run_summary_stats = {
            "total_rows": filtered_total,
            "unique_regs": filtered_unique_total,
            "not_found_rows": 0,
            "not_found_unique": 0
        }
        st.session_state["peakengine_run_summary_stats"] = run_summary_stats
        st.session_state["peakengine_run_summary_snapshot"] = df_step2_filtered

        summary_tab, runner_tab = st.tabs(["üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ö‡∏≠‡∏ó", "‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ö‡∏≠‡∏ó"])

        with summary_tab:
            col_summary_metrics = st.columns(3)
            with col_summary_metrics[0]:
                st.metric("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á", filtered_total)
            with col_summary_metrics[1]:
                st.metric("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥", filtered_unique_total)

            status_distribution = df_step2_filtered["dbd_status_display"].value_counts().rename_axis("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞").reset_index(name="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")
            type_distribution = df_step2_filtered["transfer_type"].value_counts().rename_axis("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô").reset_index(name="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô")

            st.write("**‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á)**")
            st.dataframe(status_distribution, use_container_width=True, hide_index=True)

            st.write("**‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á)**")
            st.dataframe(type_distribution, use_container_width=True, hide_index=True)

            with st.expander("‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà DBD ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", expanded=False):
                if not not_found_df.empty:
                    st.dataframe(
                        not_found_df.reset_index(drop=True)[
                            ["registration", "company_name_display", "work_category", "date", "amount", "description", "transfer_type", "dbd_info_raw"]
                        ].rename(columns={
                            "registration": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                            "company_name_display": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                            "work_category": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
                            "date": "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                            "amount": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
                            "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
                            "transfer_type": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô",
                            "dbd_info_raw": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"
                        }),
                        use_container_width=True,
                        hide_index=True
                    )
                else:
                    st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤ DBD ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô")

            st.write("**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≠‡∏ó (20 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å)**")
            st.dataframe(
                df_step2_filtered.head(20)[
                    ["registration", "company_name_display", "work_category", "date", "amount", "description", "dbd_status_display", "transfer_type", "dbd_info_raw"]
                ].rename(columns={
                    "registration": "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                    "company_name_display": "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•",
                    "work_category": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
                    "date": "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                    "amount": "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô",
                    "description": "‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢",
                    "dbd_status_display": "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD",
                    "transfer_type": "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô",
                    "dbd_info_raw": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD"
                }),
                use_container_width=True,
                hide_index=True
            )

        with runner_tab:
            processed_row_keys_list = st.session_state.get("peakengine_processed_row_keys", [])
            if not isinstance(processed_row_keys_list, list):
                processed_row_keys_list = []
            processed_row_keys_set = set(processed_row_keys_list)

            pending_row_records = [
                record for record in row_records
                if record.get("row_key") not in processed_row_keys_set
            ]
            pending_numbers = [
                record.get("registration") for record in pending_row_records
                if record.get("registration")
            ]
            pending_unique_numbers = list(dict.fromkeys(pending_numbers))

            col_stats1, col_stats2, col_stats3 = st.columns(3)
            with col_stats1:
                st.metric("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß", len(processed_row_keys_set))
            with col_stats2:
                st.metric("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠", len(pending_row_records))
            with col_stats3:
                st.metric("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥)", len(pending_unique_numbers))

            st.caption("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏≠‡∏Å‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠")

            fill_mode = st.radio(
                "‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å:",
                ["‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏Å‡∏£‡∏≠‡∏Å‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)"],
                key="peak_fill_mode",
                horizontal=True
            )

            run_summary_stats = st.session_state.get("peakengine_run_summary_stats", {})
            run_summary_df = st.session_state.get("peakengine_run_summary_snapshot")

            st.subheader("üìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó‡∏£‡∏±‡∏ô")
            st.write(
                f"- ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á: {run_summary_stats.get('total_rows', filtered_total)} "
                f"(‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ {run_summary_stats.get('unique_regs', filtered_unique_total)})"
            )
            st.write(
                "- ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà DBD ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: 0 "
                "(‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ 0)"
            )
            processed_unique_regs = set()
            for row_key in processed_row_keys_set:
                record = row_key_map.get(row_key)
                if record and record.get("registration"):
                    processed_unique_regs.add(record.get("registration"))
            st.write(f"- ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏£‡∏±‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß: {len(processed_row_keys_set)} (‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ {len(processed_unique_regs)})")
            st.write(f"- ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏≠‡∏ó‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ: {len(pending_row_records)} (‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥ {len(pending_unique_numbers)})")
            pending_not_found = [
                reg for reg in pending_unique_numbers
                if run_summary_df is not None
                and not run_summary_df.empty
                and any(
                    (run_summary_df["registration"] == reg) &
                    (
                        run_summary_df["dbd_status_display"].astype(str).str.strip() == "‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≠‡∏Å‡∏ô‡∏≤‡∏°"
                    )
                )
            ]
            if pending_not_found:
                st.write(f"- ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏à‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ DBD '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': {len(pending_not_found)}")
                st.code(", ".join(pending_not_found[:20]), language=None)
                if len(pending_not_found) > 20:
                    st.caption(f"... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(pending_not_found) - 20} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

            selected_registration = None
            selected_record = None
            if fill_mode == "‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£":
                def format_row_option(row_key: str) -> str:
                    record = row_key_map.get(row_key, {})
                    registration_value = record.get("registration", "-")
                    company_value = record.get("company_name_display", "-")
                    date_value = record.get("date", "-")
                    amount_value = record.get("amount", "-")
                    work_category_value = record.get("work_category", "-")
                    return f"{registration_value} ‚Ä¢ {company_value} ‚Ä¢ {date_value} ‚Ä¢ {amount_value} ‚Ä¢ {work_category_value}"

                available_row_keys = [
                    record.get("row_key") for record in pending_row_records
                    if record.get("row_key") is not None
                ]
                if not available_row_keys and row_records:
                    available_row_keys = [
                        record.get("row_key") for record in row_records
                        if record.get("row_key") is not None
                    ]

                if not available_row_keys:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å")
                    return

                selected_row_key = st.selectbox(
                    "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏Å:",
                    available_row_keys,
                    index=0 if available_row_keys else None,
                    key="peakengine_selected_row_key",
                    format_func=format_row_option
                )

                if not selected_row_key:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏Å")
                    return
                selected_record = row_key_map.get(selected_row_key)
                if not selected_record:
                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
                    return
                selected_registration = selected_record.get("registration")
                target_row_records = [selected_record]
            else:
                st.session_state["peakengine_selected_row_key"] = ""
                target_row_records = pending_row_records

            if st.button("‚ôªÔ∏è ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß", key="reset_peak_processed"):
                st.session_state["peakengine_processed_regs"] = []
                st.session_state["peakengine_processed_row_keys"] = []
                st.success("‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
                st.experimental_rerun()

            log_expander = st.expander("üìã Log ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", expanded=False)
            log_placeholder = log_expander.empty()
            log_messages: List[Dict[str, str]] = []

            def peak_log(message: str, status: str = "info"):
                icon_map = {
                    "info": "‚ÑπÔ∏è",
                    "success": "‚úÖ",
                    "warning": "‚ö†Ô∏è",
                    "error": "‚ùå"
                }
                log_messages.append({
                    "time": datetime.now().strftime("%H:%M:%S"),
                    "status": status,
                    "message": message
                })
                lines = []
                for entry in log_messages[-200:]:
                    icon = icon_map.get(entry["status"], "üìù")
                    lines.append(f"[{entry['time']}] {icon} {entry['message']}")
                log_placeholder.code("\n".join(lines), language=None)

            col_fill_peak, col_newpeak = st.columns(2)
            with col_fill_peak:
                if st.button("üìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PEAK", type="primary", key="fill_peak_contacts_btn"):
                    if config is None:
                        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå config.py ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö PEAKEngine ‡πÑ‡∏î‡πâ")
                        return

                    username = getattr(config, 'PEAKENGINE_USERNAME', '')
                    password = getattr(config, 'PEAKENGINE_PASSWORD', '')
                    link_company = getattr(config, 'Link_conpany', None)
                    link_receipt = getattr(config, 'Link_receipt', None)
                    headless = getattr(config, 'HEADLESS_MODE', False)

                    if not username or not password:
                        st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î PEAKENGINE_USERNAME ‡πÅ‡∏•‡∏∞ PEAKENGINE_PASSWORD ‡πÉ‡∏ô config.py ‡∏Å‡πà‡∏≠‡∏ô")
                        return

                    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö..."):
                        try:
                            from peakengine_bot import PeakEngineBot
                            bot = PeakEngineBot(use_browser=True, headless=headless)
                            _peakengine_bots.append(bot)

                            peak_log("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö PEAKEngine...", "info")
                            login_success = bot.login(
                                username,
                                password,
                                link_company=link_company,
                                link_receipt=link_receipt,
                                log_callback=peak_log
                            )

                            if not login_success:
                                st.error("‚ùå ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö PEAKEngine ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô config.py")
                                return

                            peak_log("‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô", "success")

                            if fill_mode == "‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£":
                                fill_targets = [selected_registration] if selected_registration else []
                                target_row_keys = []
                                if selected_record and selected_record.get("row_key") is not None:
                                    target_row_keys.append(selected_record.get("row_key"))
                            else:
                                if not target_row_records:
                                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏Å")
                                    return
                                fill_targets = []
                                target_row_keys = []
                                for record in target_row_records:
                                    reg_candidate = record.get("registration")
                                    row_key_candidate = record.get("row_key")
                                    if not reg_candidate or row_key_candidate is None:
                                        continue
                                    fill_targets.append(reg_candidate)
                                    target_row_keys.append(row_key_candidate)
                                if not fill_targets:
                                    st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏£‡∏≠‡∏Å‡πÑ‡∏î‡πâ")
                                    return
                                st.info(f"üîÅ ‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(fill_targets)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏≥")

                            reg_info_map = st.session_state.get("peakengine_reg_info_map", {})
                            row_payload_map_session = st.session_state.get("peakengine_row_payload_map", {})
                            fill_result = bot.fill_contact_fields(
                                fill_targets,
                                reg_info_map=reg_info_map,
                                row_keys=target_row_keys,
                                row_payload_map=row_payload_map_session,
                                log_callback=peak_log
                            )

                            if "error" in fill_result:
                                st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {fill_result['error']}")
                            else:
                                success_count = fill_result.get("success", 0)
                                total_count = fill_result.get("total", len(fill_targets))
                                error_list = fill_result.get("errors", [])
                                processed_values = fill_result.get("processed", [])

                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å", total_count)
                                with col2:
                                    st.metric("‡∏Å‡∏£‡∏≠‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", success_count)
                                with col3:
                                    st.metric("‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", len(error_list))

                                processed_row_keys_list = fill_result.get("processed_row_keys", [])
                                if processed_row_keys_list:
                                    processed_row_keys_set.update(processed_row_keys_list)
                                    st.session_state["peakengine_processed_row_keys"] = list(processed_row_keys_set)
                                if processed_values:
                                    processed_regs_list = st.session_state.get("peakengine_processed_regs", [])
                                    processed_regs_list.extend(processed_values)
                                    st.session_state["peakengine_processed_regs"] = processed_regs_list
                                receipt_links = fill_result.get("receipt_links", [])
                                if receipt_links:
                                    stored_links = st.session_state.get("peakengine_receipt_links", [])
                                    if not isinstance(stored_links, list):
                                        stored_links = []
                                    stored_links.extend(receipt_links)
                                    st.session_state["peakengine_receipt_links"] = stored_links

                                if error_list:
                                    st.warning("‚ö†Ô∏è ‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á")
                                    df_errors = pd.DataFrame(error_list)
                                    st.dataframe(df_errors, use_container_width=True)
                                else:
                                    if fill_mode == "‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£":
                                        st.success("üéâ ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
                                    else:
                                        st.success("üéâ ‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

                                dropdown_data = fill_result.get("dropdown_options", [])
                                if dropdown_data:
                                    st.subheader("üìã ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Dropdown ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏Å")
                                    for entry in dropdown_data:
                                        value = entry.get("value", "")
                                        items = entry.get("items", [])
                                        st.write(f"**‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:** {value}")
                                        if items:
                                            st.write(f"‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ({len(items)}):")
                                            st.code("\n".join(items), language=None)
                                        else:
                                            st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô dropdown")

                                plus_clicks = fill_result.get("plus_clicked", [])
                                if plus_clicks:
                                    st.info(f"üîÑ ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏¥‡∏Å '+ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏π‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô: {', '.join(plus_clicks)}")
                                elif dropdown_data:
                                    st.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏¥‡∏Å '+ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏π‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠' ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏ô dropdown")

                                selected_existing = fill_result.get("selected_existing", [])
                                if selected_existing:
                                    st.success(f"‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô: {', '.join(selected_existing)}")

                            validation_results = fill_result.get("validation", [])
                            if validation_results:
                                st.subheader("üîç ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö Excel")
                                for validation in validation_results:
                                    reg = validation.get("registration", "")
                                    st.write(f"**‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:** {reg}")
                                    overall = validation.get("overall_match", False)
                                    if overall:
                                        st.success("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Excel ‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
                                    else:
                                        st.warning("‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Excel ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á")
                                    details = validation.get("details", [])
                                    mismatch_rows = []
                                    for detail in details:
                                        field = detail.get("field", "")
                                        expected = detail.get("expected", "")
                                        actual = detail.get("actual", "")
                                        match = detail.get("match", False)
                                        status_symbol = "‚úÖ" if match else "‚ö†Ô∏è"
                                        mismatch_rows.append(f"{status_symbol} {field}\n  - Excel: {expected}\n  - ‡∏£‡∏∞‡∏ö‡∏ö: {actual}")
                                    st.code("\n".join(mismatch_rows), language=None)

                        except Exception as e:
                            st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}")
                            peak_log(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}", "error")

        stored_links = st.session_state.get("peakengine_receipt_links", [])
        if stored_links:
            st.subheader("üìÑ ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ")
            df_receipts = pd.DataFrame(stored_links)
            st.dataframe(df_receipts, use_container_width=True)
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                df_receipts.to_excel(writer, index=False, sheet_name="Receipts")
            excel_buffer.seek(0)
            st.download_button(
                label="üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à (Excel)",
                data=excel_buffer.getvalue(),
                file_name="receipt_links.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="download_receipt_links_excel"
            )

        with col_newpeak:
            if st.button("üÜï ‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏ö New Peak (‡∏ó‡∏î‡∏•‡∏≠‡∏á)", key="open_newpeak_from_excel"):
                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå New Peak..."):
                    if open_newpeak_login():
                        st.success("‚úÖ ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î Browser ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö New Peak ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
                        st.info("üëÄ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏≠‡∏ó New Peak")
                        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot..."):
                            newpeak_bot_instance = wait_for_newpeak_instance(timeout=45, poll_interval=0.5)
                            if newpeak_bot_instance and isinstance(newpeak_bot_instance, NewPeakBot):
                                st.session_state["active_newpeak_bot"] = newpeak_bot_instance
                                peak_log("‚úÖ ‡∏û‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", "success")
                            else:
                                st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log")
                                peak_log("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î", "warning")
                                return
                        df_source = st.session_state.get("peakengine_source_df")
                        if df_source is None or df_source.empty:
                            st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Excel ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
                        else:
                            amount_col = None
                            if "‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric" in df_source.columns:
                                amount_col = "‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric"
                            elif "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô" in df_source.columns:
                                amount_col = "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô"
                            type_col = "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô" if "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô" in df_source.columns else None
                            dbd_col = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD" if "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD" in df_source.columns else None
                            company_col = None
                            for candidate in ["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD", "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏à‡∏≤‡∏Å DBD", "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"]:
                                if candidate in df_source.columns:
                                    company_col = candidate
                                    break

                            if not amount_col or not type_col or not dbd_col:
                                missing_cols = []
                                if not amount_col:
                                    missing_cols.append("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô ‡∏´‡∏£‡∏∑‡∏≠ ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô_numeric")
                                if not type_col:
                                    missing_cols.append("‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô")
                                if not dbd_col:
                                    missing_cols.append("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD")
                                st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join(missing_cols)}")
                            else:
                                try:
                                    newpeak_bot = st.session_state.get("active_newpeak_bot")
                                    if not isinstance(newpeak_bot, NewPeakBot):
                                        newpeak_bot = wait_for_newpeak_instance(timeout=30, poll_interval=0.5)
                                        if isinstance(newpeak_bot, NewPeakBot):
                                            st.session_state["active_newpeak_bot"] = newpeak_bot
                                            peak_log("‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ NewPeakBot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏¥‡∏ß", "info")
                                    if not isinstance(newpeak_bot, NewPeakBot):
                                        st.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
                                        peak_log("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå NewPeakBot ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", "warning")
                                    else:
                                        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏≠‡πÉ‡∏´‡πâ NewPeakBot ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö..."):
                                            if not wait_for_newpeak_login(newpeak_bot, timeout=90, poll_interval=0.5, log_callback=peak_log):
                                                st.warning("‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö New Peak ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î")
                                                peak_log("‚ö†Ô∏è ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö New Peak ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î", "warning")
                                                return
                                        peak_log("‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ NewPeakBot", "success")
                                        tasks, skipped_records = newpeak_bot.prepare_transaction_tasks(
                                            df_source.copy(),
                                            amount_column=amount_col,
                                            type_column=type_col,
                                            dbd_column=dbd_col,
                                            company_column=company_col,
                                        )

                                        selected_registration = st.session_state.get("selected_registration_number", "")
                                        fill_mode_value = st.session_state.get("peak_fill_mode")

                                        def normalize_registration(value: Any) -> str:
                                            if value is None or (isinstance(value, float) and pd.isna(value)):
                                                return ""
                                            text = str(value).strip()
                                            digits = "".join(ch for ch in text if ch.isdigit())
                                            if len(digits) >= 13:
                                                return digits[-13:]
                                            if len(digits) == 0:
                                                return ""
                                            return digits.zfill(13)

                                        def normalize_company_name(value: Any) -> str:
                                            if value is None or (isinstance(value, float) and pd.isna(value)):
                                                return ""
                                            text = str(value).lower()
                                            replacements = [
                                                "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó",
                                                "‡∏à‡∏≥‡∏Å‡∏±‡∏î",
                                                "‡∏°‡∏´‡∏≤‡∏ä‡∏ô",
                                                "(‡∏°‡∏´‡∏≤‡∏ä‡∏ô)",
                                                "‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô",
                                                "‡∏´‡∏à‡∏Å.",
                                                "‡∏ö‡∏à‡∏Å.",
                                                "‡∏Ñ‡∏≠‡∏£‡πå‡∏õ‡∏≠‡πÄ‡∏£‡∏ä‡∏±‡πà‡∏ô",
                                            ]
                                            for token in replacements:
                                                text = text.replace(token.lower(), " ")
                                            text = re.sub(r"[\"'.,()]", " ", text)
                                            text = re.sub(r"\s+", " ", text)
                                            return text.strip()

                                        registrations_in_tasks = [
                                            normalize_registration(task.get("registration"))
                                            for task in tasks
                                            if task.get("registration")
                                        ]
                                        if registrations_in_tasks:
                                            preview_sample = ", ".join(registrations_in_tasks[:10])
                                            if len(registrations_in_tasks) > 10:
                                                preview_sample += ", ..."
                                            peak_log(
                                                f"üóÇ ‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {preview_sample}",
                                                "info",
                                            )
                                        else:
                                            peak_log(
                                                "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô)",
                                                "warning",
                                            )

                                        if fill_mode_value == "‡∏Å‡∏£‡∏≠‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£":
                                            normalized_selected = normalize_registration(selected_registration)
                                            if normalized_selected:
                                                filtered_tasks = [
                                                    task for task in tasks
                                                    if normalize_registration(task.get("registration")) == normalized_selected
                                                ]
                                                if not filtered_tasks:
                                                    # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô Excel ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
                                                    df_matches = pd.DataFrame()
                                                    selected_rows = pd.DataFrame()
                                                    if dbd_col in df_source.columns:
                                                        selected_rows = df_source[
                                                            df_source[dbd_col]
                                                            .astype(str)
                                                            .str.replace(r"\D", "", regex=True)
                                                            .str[-13:]
                                                            .fillna("")
                                                            == normalized_selected
                                                        ]
                                                        df_matches = selected_rows.copy()
                                                    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD
                                                    if df_matches.empty and company_col:
                                                        normalized_company_series = (
                                                            df_source[company_col]
                                                            .astype(str)
                                                            .apply(normalize_company_name)
                                                        )
                                                        target_names: List[str] = []
                                                        if not selected_rows.empty and company_col in selected_rows.columns:
                                                            target_names = (
                                                                selected_rows[company_col]
                                                                .astype(str)
                                                                .apply(normalize_company_name)
                                                                .unique()
                                                                .tolist()
                                                            )
                                                        if target_names:
                                                            df_matches = df_source[normalized_company_series.isin(target_names)]
                                                    if not df_matches.empty:
                                                        peak_log(
                                                            "‚ÑπÔ∏è ‡∏û‡∏ö‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô Excel ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô "
                                                            "‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô)",
                                                            "info",
                                                        )
                                                        st.info(
                                                            "‚ÑπÔ∏è ‡∏û‡∏ö‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô Excel ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô "
                                                            "‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á‡πÇ‡∏≠‡∏ô/‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD)"
                                                        )
                                                        display_cols = [
                                                            col
                                                            for col in df_matches.columns
                                                            if col
                                                            in [
                                                                "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà",
                                                                amount_col,
                                                                type_col,
                                                                dbd_col,
                                                            ]
                                                        ]
                                                        if company_col and company_col in df_matches.columns:
                                                            display_cols.insert(0, company_col)
                                                        st.dataframe(
                                                            df_matches[display_cols],
                                                            use_container_width=True,
                                                        )
                                                    peak_log(
                                                        f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô Excel ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {normalized_selected}",
                                                        "warning",
                                                    )
                                                    st.warning(
                                                        f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô Excel ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {normalized_selected}"
                                                    )
                                                    return
                                                else:
                                                    # ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏ö task ‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏µ‡∏ï‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô 13 ‡∏´‡∏•‡∏±‡∏Å
                                                    summary_df = st.session_state.get("peakengine_summary_df")
                                                    summary_registration = ""
                                                    matched_row_indices = [task.get("row_index") for task in filtered_tasks if task.get("row_index") is not None]
                                                    matched_rows = (
                                                        df_source.loc[matched_row_indices]
                                                        if matched_row_indices
                                                        else pd.DataFrame()
                                                    )
                                                    company_name_candidates = []
                                                    task_company_candidates = [
                                                        normalize_company_name(task.get("company_name"))
                                                        for task in filtered_tasks
                                                        if task.get("company_name")
                                                    ]
                                                    company_name_candidates.extend(
                                                        [name for name in task_company_candidates if name]
                                                    )
                                                    if not matched_rows.empty and company_col and company_col in matched_rows.columns:
                                                        company_name_candidates = (
                                                            matched_rows[company_col]
                                                            .astype(str)
                                                            .apply(normalize_company_name)
                                                            .tolist()
                                                        )
                                                    if isinstance(summary_df, pd.DataFrame) and not summary_df.empty:
                                                        name_column = None
                                                        for candidate_col in ["‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD", "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", "‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"]:
                                                            if candidate_col in summary_df.columns:
                                                                name_column = candidate_col
                                                                break
                                                        normalized_names = (
                                                            summary_df[name_column].astype(str).apply(normalize_company_name)
                                                            if name_column
                                                            else pd.Series(dtype=str)
                                                        )
                                                        summary_df["_normalized_name"] = normalized_names
                                                        summary_df["_reg_digits"] = (
                                                            summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô", summary_df.get("‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô"))
                                                            .astype(str)
                                                            .str.replace(r"\D", "", regex=True)
                                                            .str[-13:]
                                                            .fillna("")
                                                        )
                                                        candidates = summary_df[
                                                            summary_df["_reg_digits"] == normalized_selected
                                                        ]
                                                        if candidates.empty and company_name_candidates:
                                                            candidates = summary_df[
                                                                summary_df["_normalized_name"].isin(company_name_candidates)
                                                            ]
                                                            if not candidates.empty:
                                                                summary_registration = candidates["_reg_digits"].iloc[0]
                                                                st.success(
                                                                    f"‚úÖ ‡∏û‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏ä‡∏µ‡∏ï '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD': {summary_registration}"
                                                                )
                                                                st.dataframe(
                                                                    candidates[
                                                                        [
                                                                            col
                                                                            for col in candidates.columns
                                                                            if col
                                                                            in [
                                                                                "‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å DBD",
                                                                                "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                                                                                "‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô_‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô",
                                                                                "‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à",
                                                                                "‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞",
                                                                                "‡∏ó‡∏∏‡∏ô‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô",
                                                                            ]
                                                                        ]
                                                                    ],
                                                                    use_container_width=True,
                                                                )
                                                    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏ó‡∏µ‡πà parse ‡πÅ‡∏•‡πâ‡∏ß
                                                    details_list = [
                                                        task.get("dbd_details", {})
                                                        for task in filtered_tasks
                                                        if task.get("dbd_details")
                                                    ]
                                                    if details_list:
                                                        st.success(f"‚úÖ ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {normalized_selected}")
                                                        details_df = pd.DataFrame(details_list)
                                                        st.dataframe(details_df, use_container_width=True)
                                                        peak_log(
                                                            f"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DBD ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {normalized_selected}",
                                                            "success",
                                                        )
                                                peak_log(
                                                    f"‚ÑπÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {normalized_selected} ({len(filtered_tasks)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)",
                                                    "info",
                                                )
                                                tasks = filtered_tasks
                                            else:
                                                st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
                                                peak_log("‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏Å", "warning")
                                                return

                                        preview_df = pd.DataFrame(tasks)
                                        skipped_df = pd.DataFrame(skipped_records)

                                        with st.expander("üóÇ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏£‡∏≠‡∏Å (New Peak)", expanded=True):
                                            if preview_df.empty:
                                                st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö New Peak")
                                            else:
                                                preview_columns = [
                                                    "row_number",
                                                    "amount",
                                                    "transfer_type",
                                                    "dbd_has_data",
                                                    "registration",
                                                ]
                                                if "company_name" in preview_df.columns:
                                                    preview_columns.append("company_name")
                                                preview_columns.append("target_url")
                                                available_preview_columns = [
                                                    col for col in preview_columns if col in preview_df.columns
                                                ]
                                                st.dataframe(
                                                    preview_df[available_preview_columns],
                                                    use_container_width=True,
                                                )
                                        if not skipped_df.empty:
                                            with st.expander("‚ö†Ô∏è ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏° (New Peak)", expanded=False):
                                                st.dataframe(skipped_df, use_container_width=True)

                                        peak_log("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö New Peak...", "info")
                                        result = newpeak_bot.process_excel_transactions(
                                            df_source.copy(),
                                            amount_column=amount_col,
                                            type_column=type_col,
                                            dbd_column=dbd_col,
                                            company_column=company_col,
                                            log_callback=peak_log,
                                            prepared_tasks=tasks,
                                            skipped_info=skipped_records,
                                        )
                                        if "error" in result:
                                            st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ: {result['error']}")
                                        else:
                                            st.success("‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö New Peak ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                                            col_np1, col_np2, col_np3 = st.columns(3)
                                            with col_np1:
                                                st.metric("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", result.get("processed", 0))
                                            with col_np2:
                                                st.metric("‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏°", result.get("skipped", 0))
                                            with col_np3:
                                                st.metric("‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î", len(result.get("errors", [])))

                                            skipped_details = result.get("skipped_details", [])
                                            if skipped_details:
                                                with st.expander("‚ÑπÔ∏è ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç", expanded=False):
                                                    st.dataframe(
                                                        pd.DataFrame(skipped_details),
                                                        use_container_width=True,
                                                    )
                                            if result.get("errors"):
                                                st.warning("‚ö†Ô∏è ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î")
                                                st.dataframe(pd.DataFrame(result["errors"]), use_container_width=True)
                                except Exception as exc:
                                    st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• New Peak: {exc}")
                                    peak_log(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• New Peak: {exc}", "error")
                    else:
                        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á NewPeakBot ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå config.py")

def main():
    st.title("üè¶ ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô Excel")
    st.markdown("---")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏ô‡∏™‡πÅ‡∏ï‡∏ô‡∏ã‡πå‡∏Ç‡∏≠‡∏á BankPDFReader
    reader = BankPDFReader()
    
    # Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
    st.sidebar.header("‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤
    st.sidebar.subheader("üìë ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤")
    page = st.sidebar.radio(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:",
        ["üìÑ Statement", "ü§ñ Bot ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡πå", "üßæ Bot ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à"],
        help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
    )
    
    st.sidebar.markdown("---")
    
    # ‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PeakEngine
    st.sidebar.subheader("üåê ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö")
    if st.sidebar.button("üîê ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤ PeakEngine Login", use_container_width=True, help="‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PeakEngine Login, ‡∏Å‡∏£‡∏≠‡∏Å username/password, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å config.py", key="open_peakengine_btn"):
        with st.sidebar:
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö PeakEngine, ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Ñ‡πå..."):
                if open_peakengine_login():
                    st.success("‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö, ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° Login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                    st.info("üëÄ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Browser ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞ login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡πÅ‡∏•‡∏∞ navigate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥")
                    if config and hasattr(config, 'PEAKENGINE_USERNAME') and config.PEAKENGINE_USERNAME:
                        st.info(f"üìß Username: {config.PEAKENGINE_USERNAME}")
                    if config:
                        if hasattr(config, 'Link_conpany'):
                            st.info(f"üîó Link_conpany: {config.Link_conpany}")
                        if hasattr(config, 'Link_receipt'):
                            st.info(f"üîó Link_receipt: {config.Link_receipt}")
                else:
                    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö, login, ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° PEAK (Deprecated) ‡∏´‡∏£‡∏∑‡∏≠ navigate ‡πÑ‡∏î‡πâ")
    if st.sidebar.button("üÜï ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤ New Peak Login", use_container_width=True, help="‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö https://secure.peakaccount.com ‡∏î‡πâ‡∏ß‡∏¢ NewPeakBot ‡πÅ‡∏•‡∏∞ navigate ‡∏ï‡∏≤‡∏° config.py", key="open_newpeak_btn"):
        with st.sidebar:
            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö New Peak, ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå..."):
                if open_newpeak_login():
                    st.success("‚úÖ ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î Browser ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö New Peak ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! ‡∏î‡∏π‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ö‡∏≠‡∏ó")
                    if config:
                        username = getattr(config, "NEWPEAK_USERNAME", getattr(config, "PEAKENGINE_USERNAME", ""))
                        if username:
                            st.info(f"üìß Username: {username}")
                        if hasattr(config, "Link_compay_newpeak"):
                            st.info(f"üîó Link_compay_newpeak: {getattr(config, 'Link_compay_newpeak')}")
                        if hasattr(config, "Link_receipt_newpeak"):
                            st.info(f"üîó Link_receipt_newpeak: {getattr(config, 'Link_receipt_newpeak')}")
                else:
                    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á NewPeakBot ‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå config.py")
    
    st.sidebar.markdown("---")
    
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ö‡∏≠‡∏ó DBD (‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏≠‡∏ó)
    use_browser_mode = False
    if "Bot" in page:
        st.sidebar.subheader("ü§ñ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ö‡∏≠‡∏ó DBD")
        use_browser_mode = st.sidebar.checkbox(
            "üåê ‡πÉ‡∏ä‡πâ Playwright Browser (‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠)",
            value=True,  # ‡πÄ‡∏õ‡∏¥‡∏î browser ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
                help="‡πÄ‡∏õ‡∏¥‡∏î Playwright Chromium Browser ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå",
                key="use_browser_checkbox"
        )
    
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô session state
        st.session_state['use_browser_mode'] = use_browser_mode
        st.session_state['headless_mode'] = False  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á browser ‡πÄ‡∏™‡∏°‡∏≠
    
    if use_browser_mode:
        st.sidebar.success("üëÄ **Browser ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå!**")
        st.sidebar.info("üí° **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** ‡∏î‡∏π Browser window ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ - ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏ó‡∏ó‡∏≥‡∏á‡∏≤‡∏ô")
        st.sidebar.markdown("""
        **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:**
        1. üåê ‡πÄ‡∏õ‡∏¥‡∏î Browser
        2. üìç ‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö DBD DataWarehouse
        3. üîç ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        4. ‚å®Ô∏è ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó
        5. üîò ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        6. üìä ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        """)

        if st.sidebar.button("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î Playwright Browser", help="‡πÄ‡∏õ‡∏¥‡∏î Chromium ‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á playwright ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ", key="test_browser_btn"):
            with st.sidebar:
                with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î Playwright Chromium..."):
                    if test_playwright_browser():
                        st.success("‚úÖ ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏õ‡∏¥‡∏î Playwright ‡πÅ‡∏•‡πâ‡∏ß!")
                        st.info("üëÄ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á Chromium ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤")
                    else:
                        st.error("‚ùå ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Playwright ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
                        st.sidebar.markdown("---")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ (‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ Statement)
    if page == "üìÑ Statement":
        st.sidebar.subheader("üè¶ ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ PDF")
        bank_names = list(reader.bank_configs.keys())
        selected_bank = st.sidebar.selectbox(
            "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£:",
            bank_names,
                help="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì",
                key="bank_select"
            )
    else:
        selected_bank = None
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤
    if page == "üìÑ Statement":
        uploaded_file, selected_bank = render_statement_page(reader, selected_bank)
    elif page == "ü§ñ Bot ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡πå":
        render_dbd_bot_page(reader)
        uploaded_file = None
    elif page == "üßæ Bot ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à":
        render_receipt_bot_page()
        uploaded_file = None
    
    # Footer
    st.markdown("---")
    st.header("‚ÑπÔ∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üè¶ ‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö")
        bank_names = list(reader.bank_configs.keys())
        for bank in bank_names:
            st.write(f"‚Ä¢ {bank}")
    
    with col2:
        st.subheader("‚ú® ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå")
        features = [
            "‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥",
            "‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Excel",
            "‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡πà‡∏á",
            "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥",
            "‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
            "UI ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"
        ]
        for feature in features:
            st.write(f"‚Ä¢ {feature}")
    
    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            <p>üè¶ Bank PDF to Excel Converter | ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Streamlit</p>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
        
        
